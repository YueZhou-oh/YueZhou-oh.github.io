<html>
<head>
  <title>Evernote Export</title>
  <basefont face="微软雅黑" size="2" />
  <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
  <meta name="exporter-version" content="YXBJ Windows/602628 (zh-CN, DDL); Windows/10.0.0 (Win64); EDAMVersion=V2;"/>
  <meta name="content-class" content="yinxiang.markdown"/>
  <style>
    body, td {
      font-family: 微软雅黑;
      font-size: 10pt;
    }
  </style>
</head>
<body>
<a name="1143"/>

<div><span><div style="font-size: 14px; margin: 0; padding: 0; width: 100%;"><pre style="line-height: 160%; box-sizing: content-box; border: 0; border-radius: 0; margin: 2px 0 8px; background-color: #f5f7f8;"><code style="display: block; overflow-x: auto; background: #1e1e1e; line-height: 160%; box-sizing: content-box; border: 0; border-radius: 0; letter-spacing: -.3px; padding: 18px; color: #f4f4f4; white-space: pre-wrap;">LSTM forward and backward derivation:
https://datascience-enthusiast.com/DL/Building_a_Recurrent_Neural_Network-Step_by_Step_v1.html

</code></pre>
<h3 style="line-height: 160%; box-sizing: content-box; font-weight: 700; font-size: 27px; color: #333;">RNN</h3>
<blockquote style="line-height: 160%; box-sizing: content-box; margin: 15px 0; border-left: 4px solid #ddd; padding: 0 15px; color: #777;">
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333; margin-top: 0; margin-bottom: 0;">deep learning book : https://srdas.github.io/DLBook/RNNs.html#LSTMs</p>
</blockquote>
<h4 style="line-height: 160%; box-sizing: content-box; font-size: 20px; color: #333;"><a href="https://www.academia.edu/30351853/The_utility_driven_dynamic_error_propagation_network" style="line-height: 160%; box-sizing: content-box; text-decoration: underline; color: #5286bc;">Vanilla RNN (short-term memory)</a></h4>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;">RNN,1987</p>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;"><a href="https://www.nature.com/articles/323533a0" style="line-height: 160%; box-sizing: content-box; text-decoration: underline; color: #5286bc;">BP-Nature</a>，1986</p>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;"><a href="https://www.sciencedirect.com/science/article/abs/pii/089360808890007X" style="line-height: 160%; box-sizing: content-box; text-decoration: underline; color: #5286bc;">BPTT</a>,1988</p>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.56.7941&amp;rep=rep1&amp;type=pdf" style="line-height: 160%; box-sizing: content-box; text-decoration: underline; color: #5286bc;">Truncated BPTT</a>, 1990</p>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;"><a href="https://arxiv.org/pdf/1503.04069.pdf" style="line-height: 160%; box-sizing: content-box; text-decoration: underline; color: #5286bc;">Truncated BPTT 2 </a></p>
<ol style="line-height: 160%; box-sizing: content-box; display: block; padding-left: 30px; margin: 6px 0 10px; color: #333; list-style-type: decimal;">
<li style="line-height: 160%; box-sizing: content-box;">1987，Real-Time Recurrent Learning, 提出Vanilla RNN (short term memory)， 有限的记忆，难以捕捉长期的时间关联</li>
<li style="line-height: 160%; box-sizing: content-box;">1988，Back-Propagation Through Time，无法很好处理权重指数级爆炸或梯度消失问题</li>
</ol>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;">因此，此时RNN未得到较大关注。<br/>
RNN结构</p>
<ol style="line-height: 160%; box-sizing: content-box; display: block; padding-left: 30px; margin: 6px 0 10px; color: #333; list-style-type: decimal;">
<li style="line-height: 160%; box-sizing: content-box;">Networks with feedback loops (recurrent edges) vs feedforward neural network</li>
<li style="line-height: 160%; box-sizing: content-box;">Output at current time step depends on current input as well as previous state (via recurrent edges)</li>
</ol>
<blockquote style="line-height: 160%; box-sizing: content-box; margin: 15px 0; border-left: 4px solid #ddd; padding: 0 15px; color: #777;">
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333; margin-top: 0; margin-bottom: 0;">example<br/>
<img src="RNN_files/Image.png" type="image/png" data-filename="Image.png"/><br/>
<img src="RNN_files/Image [1].png" type="image/png" data-filename="Image.png"/><br/>
<img src="RNN_files/Image [2].png" type="image/png" data-filename="Image.png"/></p>
</blockquote>
<h4 style="line-height: 160%; box-sizing: content-box; font-size: 20px; color: #333;"><a href="https://www.researchgate.net/publication/13853244_Long_Short-term_Memory" style="line-height: 160%; box-sizing: content-box; text-decoration: underline; color: #5286bc;">LSTM</a></h4>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;">1997，Long short term memory提出</p>
<ol style="line-height: 160%; box-sizing: content-box; display: block; padding-left: 30px; margin: 6px 0 10px; color: #333; list-style-type: decimal;">
<li style="line-height: 160%; box-sizing: content-box;">长短时记忆，记忆可控</li>
<li style="line-height: 160%; box-sizing: content-box;">An appropriate gradient-based learning algorithm</li>
</ol>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;">有效改善以上问题，成为往后RNN发展的基础结构，在memory cell和network architecture上的变形，以适用实际数据和业务场景</p>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;"><img src="RNN_files/Image [3].png" type="image/png" data-filename="Image.png"/></p>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;">标准的 RNN 中，这个重复的模块只有一个非常简单的结构，例如一个 tanh 层。LSTM 同样是这样的结构，但是重复的模块拥有一个不同的结构，重复模块包含四个交互的gate。<br/>
主要大的区别是，采用一个叫 “细胞状态（state）” 的通道贯穿了整个时间序列。通过精心设计的称作 “门” 的结构来去除或增加信息到细胞状态的能力。组成部分  <br/>
1）&quot; 忘记门”；<br/>
2）“输入门” 的打开关闭也是由当前输入和上一个时间点的输出决定的。<br/>
3）“输出门”，控制输出多少，最终仅仅会输出确定输出的那部分。<br/>
4）状态门：让几个 “门” 的输入数据除了正常的输入数据和上一个时刻的输出以外，再接受 “细胞状态” 的输入。</p>
<h4 style="line-height: 160%; box-sizing: content-box; font-size: 20px; color: #333;">Bidirectional RNN</h4>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;">1997，<a href="https://ieeexplore.ieee.org/document/650093" style="line-height: 160%; box-sizing: content-box; text-decoration: underline; color: #5286bc;">Bidirectional Recurrent Neural Network</a><br/>
2013, <a href="https://arxiv.org/pdf/1303.5778.pdf" style="line-height: 160%; box-sizing: content-box; text-decoration: underline; color: #5286bc;">BLSTM</a></p>
<blockquote style="line-height: 160%; box-sizing: content-box; margin: 15px 0; border-left: 4px solid #ddd; padding: 0 15px; color: #777;">
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333; margin-top: 0;">经典语音识别系统通常有如下几个组成部分：特征提取，如利用输入的waveform提取MFCC特征，然后再经过三个独立的模型再求得它们概率的乘积得到总的概率：</p>
<ol style="line-height: 160%; box-sizing: content-box; display: block; padding-left: 30px; margin: 6px 0 10px; color: #333; list-style-type: decimal;">
<li style="line-height: 160%; box-sizing: content-box;">acoustic model 即根据之前提取的特征预测每个对应的音素(phoneme)，传统上用GMM Gaussian Mixture Model。</li>
<li style="line-height: 160%; box-sizing: content-box;">pronunciation model即根据音素组合成词语的发音, 传统上用一些pronunciation table。</li>
<li style="line-height: 160%; box-sizing: content-box;">language model即根据发音预测对应的文本, 传统上用一些n-gram model。<br/>
<img src="RNN_files/Image [4].png" type="image/png" data-filename="Image.png"/></li>
</ol>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333; margin-bottom: 0;">神经网络技术发展后，特征提取又可用CNN来做，其他部分也可用DNN或一些RNN结构如LSTM来做。<br/>
但是这三部分模型还是相互独立训练的，这使得训练过程异常复杂，我们希望能用一个<strong style="line-height: 160%; box-sizing: content-box; font-weight: 700;">end-to-end</strong>的模型来包含所有的步骤：可输入语音或其频谱而直接产出文本从而简化训练过程。<br/>
End-to-end模型：CTC，RNN-T，LAS，et. al.</p>
</blockquote>
<h4 style="line-height: 160%; box-sizing: content-box; font-size: 20px; color: #333;"><a href="https://www.cs.toronto.edu/~graves/icml_2006.pdf" style="line-height: 160%; box-sizing: content-box; text-decoration: underline; color: #5286bc;">CTC</a></h4>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;">2006，ICML，Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks，主要是为了解决利用LSTM训练时需要目标label与输入的每一帧需要alignment的问题</p>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;">2009年，一个CTC-LSTM ()网络赢得了多项连笔手写识别竞赛 (The International Conference on Document Analysis and Recognition (ICDAR))，成为第一个赢得模式识别竞赛的RNN。RNN逐渐应用于pattern recognition各领域</p>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;">CTC is a type of neural network output and associated scoring function, for training recurrent neural networks (RNNs) such as LSTM networks to tackle sequence problems where the timing is variable.</p>
<blockquote style="line-height: 160%; box-sizing: content-box; margin: 15px 0; border-left: 4px solid #ddd; padding: 0 15px; color: #777;">
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333; margin-top: 0; margin-bottom: 0;">CTC对于语音识别的声学建模带来了极大的好处<br/>
1）化繁为简，不在需要强制对齐，可以使用文本序列本身来进行学习训练<br/>
2）加速解码，大量Blank的存在，使得模型在解码过程中可以使用跳帧操作，因此大大加速了解码过程。</p>
</blockquote>
<blockquote style="line-height: 160%; box-sizing: content-box; margin: 15px 0; border-left: 4px solid #ddd; padding: 0 15px; color: #777;">
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333; margin-top: 0; margin-bottom: 0;">但是CTC模型仍然存在着很多的问题，其中最显著的就是CTC假设模型的输出之间是条件独立的。这个基本假设与语音识别任务之前存在着一定程度的背离。此外，CTC模型并不具有语言建模能力，同时也并没有真正的实现端到端的联合优化。</p>
</blockquote>
<h4 style="line-height: 160%; box-sizing: content-box; font-size: 20px; color: #333;">RNN-T</h4>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;">基于CTC的改进，语音识别,<br/>
2012，<a href="https://arxiv.org/abs/1211.3711" style="line-height: 160%; box-sizing: content-box; text-decoration: underline; color: #5286bc;">Sequence Transduction with Recurrent Neural Network</a><br/>
2014，<a href="https://arxiv.org/abs/1409.3215" style="line-height: 160%; box-sizing: content-box; text-decoration: underline; color: #5286bc;">Sequence to Sequence Learning with Neural Networks</a></p>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;">针对CTC的不足，Alex Graves在2012年左右提出了RNN-T模型，RNN-T模型巧妙的将语言模型声学模型整合在一起，同时进行联合优化，是一种理论上相对完美的模型结构。<br/>
RNN-T模型引入了TranscriptionNet也就是图中的Encoder（可以使用任何声学模型的结构），相当于声学模型部分，图中的PredictionNet实际上相当于语言模型（可以使用单向的循环神经网络来构建）。<br/>
模型中比较新奇，同时也是最重要的结构就是联合网络Joint Net，一般可以使用前向网络来进行建模。<br/>
联合网络的作用就是将语言模型和声学模型的状态通过某种思路结合在一起，可以是拼接操作，也可以是直接相加等，考虑到语言模型和声学模型可能有不同的权重问题，似乎拼接操作更加合理一些。</p>
<h4 style="line-height: 160%; box-sizing: content-box; font-size: 20px; color: #333;"><a href="https://arxiv.org/abs/1406.1078" style="line-height: 160%; box-sizing: content-box; text-decoration: underline; color: #5286bc;">GRU</a></h4>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;">2014，a long short-term memory (LSTM) with a forget gate</p>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;">相比LSTM，使用GRU能够达到相当的效果，并且相比之下更容易进行训练，能够很大程度上提高训练效率<br/>
<img src="RNN_files/Image [5].png" type="image/png" data-filename="Image.png"/></p>
<h4 style="line-height: 160%; box-sizing: content-box; font-size: 20px; color: #333;"><a href="https://aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/9745" style="line-height: 160%; box-sizing: content-box; text-decoration: underline; color: #5286bc;">Recurrent convolutional NN</a></h4>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;">2015 AAAI，Recurrent Convolutional Neural Networks for Text Classification</p>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;">本文要解决的问题是文本分类，文本分类最关键的问题是特征表示，传统的方法经常会忽略上下文信息和词序，无法捕捉到词义。</p>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;">为了解决RNN的有偏性问题，有的研究者提出了用CNN（卷积神经网络）来表示文本，并且时间复杂度也是O(n)，但是CNN存在一个缺陷，卷积窗口的大小是固定的，并且这个窗口大小如何设置是一个问题，如果设置小了，则会损失有效信息，如果设置大了，会增加很多的参数。<br/>
于是，针对上述模型存在的问题，本文提出了RCNN（循环卷积神经网络）模型<br/>
<img src="RNN_files/Image [6].png" type="image/png" data-filename="Image.png"/></p>
<h4 style="line-height: 160%; box-sizing: content-box; font-size: 20px; color: #333;"><a href="https://arxiv.org/abs/1810.04805" style="line-height: 160%; box-sizing: content-box; text-decoration: underline; color: #5286bc;">BERT</a></h4>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;">2019，BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding，语义理解</p>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;">BERT（Bidirectional Encoder Representations from Transformers）近期提出之后，作为一个Word2Vec的替代者，其在NLP领域的11个方向大幅刷新了精度，可以说是近年来自残差网络最优突破性的一项技术了</p>
<ol style="line-height: 160%; box-sizing: content-box; display: block; padding-left: 30px; margin: 6px 0 10px; color: #333; list-style-type: decimal;">
<li style="line-height: 160%; box-sizing: content-box;">使用了Transformer [2]作为算法的主要框架，Transformer能更彻底的捕捉语句中的双向关系；</li>
<li style="line-height: 160%; box-sizing: content-box;">使用了Mask Language Model(MLM) [3] 和 Next Sentence Prediction(NSP) 的多任务训练目标；</li>
<li style="line-height: 160%; box-sizing: content-box;">使用更强大的机器训练更大规模的数据，使BERT的结果达到了全新的高度，并且Google开源了BERT模型，用户可以直接使用BERT作为Word2Vec的转换矩阵并高效的将其应用到自己的任务中。</li>
</ol>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;">BERT的本质上是通过在海量的语料的基础上运行自监督学习方法为单词学习一个好的特征表示，所谓自监督学习是指在没有人工标注的数据上运行的监督学习。<br/>
在以后特定的NLP任务中，我们可以直接使用BERT的特征表示作为该任务的词嵌入特征。所以BERT提供的是一个供其它任务迁移学习的模型，该模型可以根据任务微调或者固定之后作为特征提取器。</p>
<h4 style="line-height: 160%; box-sizing: content-box; font-size: 20px; color: #333;"><a href="https://arxiv.org/abs/2005.14165" style="line-height: 160%; box-sizing: content-box; text-decoration: underline; color: #5286bc;">GPT-3</a></h4>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;">https://github.com/openai/gpt-3<br/>
2020，<br/>
Language Models are Few-Shot Learners</p>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;">具有1,750亿个参数的自然语言深度学习模型，比以前的版本GPT-2高100倍<br/>
该模型经过了将近0.5万亿个单词的预训练，并且在不进行微调的情况下，可以在多个NLP基准上达到最先进的性能。</p>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;">GPT-3 最令人惊讶的还是模型体量，它使用的最大数据集在处理前容量达到了 45TB。</p>
</div><center style="display:none !important;visibility:collapse !important;height:0 !important;white-space:nowrap;width:100%;overflow:hidden">%60%60%60%0ALSTM%20forward%20and%20backward%20derivation%3A%0Ahttps%3A%2F%2Fdatascience-enthusiast.com%2FDL%2FBuilding_a_Recurrent_Neural_Network-Step_by_Step_v1.html%0A%0A%60%60%60%0A%0A%23%23%23%20RNN%0A%3Edeep%20learning%20book%20%3A%20https%3A%2F%2Fsrdas.github.io%2FDLBook%2FRNNs.html%23LSTMs%0A%23%23%23%23%20%5BVanilla%20RNN%20(short-term%20memory)%5D(https%3A%2F%2Fwww.academia.edu%2F30351853%2FThe_utility_driven_dynamic_error_propagation_network)%0A%0ARNN%2C1987%0A%0A%5BBP-Nature%5D(https%3A%2F%2Fwww.nature.com%2Farticles%2F323533a0)%EF%BC%8C1986%0A%5BBPTT%5D(https%3A%2F%2Fwww.sciencedirect.com%2Fscience%2Farticle%2Fabs%2Fpii%2F089360808890007X)%2C1988%0A%5BTruncated%20BPTT%5D(http%3A%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fdownload%3Fdoi%3D10.1.1.56.7941%26rep%3Drep1%26type%3Dpdf)%2C%201990%0A%5BTruncated%20BPTT%202%20%5D(https%3A%2F%2Farxiv.org%2Fpdf%2F1503.04069.pdf)%0A%0A1.%201987%EF%BC%8CReal-Time%20Recurrent%20Learning%2C%20%E6%8F%90%E5%87%BAVanilla%20RNN%20(short%20term%20memory)%EF%BC%8C%20%E6%9C%89%E9%99%90%E7%9A%84%E8%AE%B0%E5%BF%86%EF%BC%8C%E9%9A%BE%E4%BB%A5%E6%8D%95%E6%8D%89%E9%95%BF%E6%9C%9F%E7%9A%84%E6%97%B6%E9%97%B4%E5%85%B3%E8%81%94%0A2.%201988%EF%BC%8CBack-Propagation%20Through%20Time%EF%BC%8C%E6%97%A0%E6%B3%95%E5%BE%88%E5%A5%BD%E5%A4%84%E7%90%86%E6%9D%83%E9%87%8D%E6%8C%87%E6%95%B0%E7%BA%A7%E7%88%86%E7%82%B8%E6%88%96%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E9%97%AE%E9%A2%98%0A%20%0A%20%E5%9B%A0%E6%AD%A4%EF%BC%8C%E6%AD%A4%E6%97%B6RNN%E6%9C%AA%E5%BE%97%E5%88%B0%E8%BE%83%E5%A4%A7%E5%85%B3%E6%B3%A8%E3%80%82%0A%20RNN%E7%BB%93%E6%9E%84%0A%201.%20Networks%20with%20feedback%20loops%20(recurrent%20edges)%20vs%20feedforward%20neural%20network%0A%202.%20Output%20at%20current%20time%20step%20depends%20on%20current%20input%20as%20well%20as%20previous%20state%20(via%20recurrent%20edges)%0A%0A%0A%3Eexample%0A%3E!%5B1dab24dcf7173580341dc97d1fcbdd43.png%5D(en-resource%3A%2F%2Fdatabase%2F1149%3A0)%0A!%5B992f8f0982bc4956d12d96719423f56f.png%5D(en-resource%3A%2F%2Fdatabase%2F1147%3A0)%0A!%5B9a1d220729b699a0fb09732e3a461c60.png%5D(en-resource%3A%2F%2Fdatabase%2F1151%3A0)%0A%0A%0A%23%23%23%23%20%5BLSTM%5D(https%3A%2F%2Fwww.researchgate.net%2Fpublication%2F13853244_Long_Short-term_Memory)%0A1997%EF%BC%8CLong%20short%20term%20memory%E6%8F%90%E5%87%BA%0A1.%20%E9%95%BF%E7%9F%AD%E6%97%B6%E8%AE%B0%E5%BF%86%EF%BC%8C%E8%AE%B0%E5%BF%86%E5%8F%AF%E6%8E%A7%0A2.%20An%20appropriate%20gradient-based%20learning%20algorithm%0A%0A%E6%9C%89%E6%95%88%E6%94%B9%E5%96%84%E4%BB%A5%E4%B8%8A%E9%97%AE%E9%A2%98%EF%BC%8C%E6%88%90%E4%B8%BA%E5%BE%80%E5%90%8ERNN%E5%8F%91%E5%B1%95%E7%9A%84%E5%9F%BA%E7%A1%80%E7%BB%93%E6%9E%84%EF%BC%8C%E5%9C%A8memory%20cell%E5%92%8Cnetwork%20architecture%E4%B8%8A%E7%9A%84%E5%8F%98%E5%BD%A2%EF%BC%8C%E4%BB%A5%E9%80%82%E7%94%A8%E5%AE%9E%E9%99%85%E6%95%B0%E6%8D%AE%E5%92%8C%E4%B8%9A%E5%8A%A1%E5%9C%BA%E6%99%AF%0A%0A!%5B3a436414930323eed8cab9bb5d49351d.png%5D(en-resource%3A%2F%2Fdatabase%2F1153%3A0)%0A%0A%0A%E6%A0%87%E5%87%86%E7%9A%84%20RNN%20%E4%B8%AD%EF%BC%8C%E8%BF%99%E4%B8%AA%E9%87%8D%E5%A4%8D%E7%9A%84%E6%A8%A1%E5%9D%97%E5%8F%AA%E6%9C%89%E4%B8%80%E4%B8%AA%E9%9D%9E%E5%B8%B8%E7%AE%80%E5%8D%95%E7%9A%84%E7%BB%93%E6%9E%84%EF%BC%8C%E4%BE%8B%E5%A6%82%E4%B8%80%E4%B8%AA%20tanh%20%E5%B1%82%E3%80%82LSTM%20%E5%90%8C%E6%A0%B7%E6%98%AF%E8%BF%99%E6%A0%B7%E7%9A%84%E7%BB%93%E6%9E%84%EF%BC%8C%E4%BD%86%E6%98%AF%E9%87%8D%E5%A4%8D%E7%9A%84%E6%A8%A1%E5%9D%97%E6%8B%A5%E6%9C%89%E4%B8%80%E4%B8%AA%E4%B8%8D%E5%90%8C%E7%9A%84%E7%BB%93%E6%9E%84%EF%BC%8C%E9%87%8D%E5%A4%8D%E6%A8%A1%E5%9D%97%E5%8C%85%E5%90%AB%E5%9B%9B%E4%B8%AA%E4%BA%A4%E4%BA%92%E7%9A%84gate%E3%80%82%0A%E4%B8%BB%E8%A6%81%E5%A4%A7%E7%9A%84%E5%8C%BA%E5%88%AB%E6%98%AF%EF%BC%8C%E9%87%87%E7%94%A8%E4%B8%80%E4%B8%AA%E5%8F%AB%20%E2%80%9C%E7%BB%86%E8%83%9E%E7%8A%B6%E6%80%81%EF%BC%88state%EF%BC%89%E2%80%9D%20%E7%9A%84%E9%80%9A%E9%81%93%E8%B4%AF%E7%A9%BF%E4%BA%86%E6%95%B4%E4%B8%AA%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E3%80%82%E9%80%9A%E8%BF%87%E7%B2%BE%E5%BF%83%E8%AE%BE%E8%AE%A1%E7%9A%84%E7%A7%B0%E4%BD%9C%20%E2%80%9C%E9%97%A8%E2%80%9D%20%E7%9A%84%E7%BB%93%E6%9E%84%E6%9D%A5%E5%8E%BB%E9%99%A4%E6%88%96%E5%A2%9E%E5%8A%A0%E4%BF%A1%E6%81%AF%E5%88%B0%E7%BB%86%E8%83%9E%E7%8A%B6%E6%80%81%E7%9A%84%E8%83%BD%E5%8A%9B%E3%80%82%E7%BB%84%E6%88%90%E9%83%A8%E5%88%86%E2%80%83%E2%80%83%0A1%EF%BC%89%22%20%E5%BF%98%E8%AE%B0%E9%97%A8%E2%80%9D%EF%BC%9B%0A2%EF%BC%89%E2%80%9C%E8%BE%93%E5%85%A5%E9%97%A8%E2%80%9D%20%E7%9A%84%E6%89%93%E5%BC%80%E5%85%B3%E9%97%AD%E4%B9%9F%E6%98%AF%E7%94%B1%E5%BD%93%E5%89%8D%E8%BE%93%E5%85%A5%E5%92%8C%E4%B8%8A%E4%B8%80%E4%B8%AA%E6%97%B6%E9%97%B4%E7%82%B9%E7%9A%84%E8%BE%93%E5%87%BA%E5%86%B3%E5%AE%9A%E7%9A%84%E3%80%82%0A3%EF%BC%89%E2%80%9C%E8%BE%93%E5%87%BA%E9%97%A8%E2%80%9D%EF%BC%8C%E6%8E%A7%E5%88%B6%E8%BE%93%E5%87%BA%E5%A4%9A%E5%B0%91%EF%BC%8C%E6%9C%80%E7%BB%88%E4%BB%85%E4%BB%85%E4%BC%9A%E8%BE%93%E5%87%BA%E7%A1%AE%E5%AE%9A%E8%BE%93%E5%87%BA%E7%9A%84%E9%82%A3%E9%83%A8%E5%88%86%E3%80%82%0A4%EF%BC%89%E7%8A%B6%E6%80%81%E9%97%A8%EF%BC%9A%E8%AE%A9%E5%87%A0%E4%B8%AA%20%E2%80%9C%E9%97%A8%E2%80%9D%20%E7%9A%84%E8%BE%93%E5%85%A5%E6%95%B0%E6%8D%AE%E9%99%A4%E4%BA%86%E6%AD%A3%E5%B8%B8%E7%9A%84%E8%BE%93%E5%85%A5%E6%95%B0%E6%8D%AE%E5%92%8C%E4%B8%8A%E4%B8%80%E4%B8%AA%E6%97%B6%E5%88%BB%E7%9A%84%E8%BE%93%E5%87%BA%E4%BB%A5%E5%A4%96%EF%BC%8C%E5%86%8D%E6%8E%A5%E5%8F%97%20%E2%80%9C%E7%BB%86%E8%83%9E%E7%8A%B6%E6%80%81%E2%80%9D%20%E7%9A%84%E8%BE%93%E5%85%A5%E3%80%82%0A%0A%0A%23%23%23%23%20Bidirectional%20RNN%0A1997%EF%BC%8C%5BBidirectional%20Recurrent%20Neural%20Network%5D(https%3A%2F%2Fieeexplore.ieee.org%2Fdocument%2F650093)%0A2013%2C%20%5BBLSTM%5D(https%3A%2F%2Farxiv.org%2Fpdf%2F1303.5778.pdf)%0A%0A%3E%E7%BB%8F%E5%85%B8%E8%AF%AD%E9%9F%B3%E8%AF%86%E5%88%AB%E7%B3%BB%E7%BB%9F%E9%80%9A%E5%B8%B8%E6%9C%89%E5%A6%82%E4%B8%8B%E5%87%A0%E4%B8%AA%E7%BB%84%E6%88%90%E9%83%A8%E5%88%86%EF%BC%9A%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%EF%BC%8C%E5%A6%82%E5%88%A9%E7%94%A8%E8%BE%93%E5%85%A5%E7%9A%84waveform%E6%8F%90%E5%8F%96MFCC%E7%89%B9%E5%BE%81%EF%BC%8C%E7%84%B6%E5%90%8E%E5%86%8D%E7%BB%8F%E8%BF%87%E4%B8%89%E4%B8%AA%E7%8B%AC%E7%AB%8B%E7%9A%84%E6%A8%A1%E5%9E%8B%E5%86%8D%E6%B1%82%E5%BE%97%E5%AE%83%E4%BB%AC%E6%A6%82%E7%8E%87%E7%9A%84%E4%B9%98%E7%A7%AF%E5%BE%97%E5%88%B0%E6%80%BB%E7%9A%84%E6%A6%82%E7%8E%87%EF%BC%9A%0A%3E1.%20acoustic%20model%20%E5%8D%B3%E6%A0%B9%E6%8D%AE%E4%B9%8B%E5%89%8D%E6%8F%90%E5%8F%96%E7%9A%84%E7%89%B9%E5%BE%81%E9%A2%84%E6%B5%8B%E6%AF%8F%E4%B8%AA%E5%AF%B9%E5%BA%94%E7%9A%84%E9%9F%B3%E7%B4%A0(phoneme)%EF%BC%8C%E4%BC%A0%E7%BB%9F%E4%B8%8A%E7%94%A8GMM%20Gaussian%20Mixture%20Model%E3%80%82%0A%3E2.%20pronunciation%20model%E5%8D%B3%E6%A0%B9%E6%8D%AE%E9%9F%B3%E7%B4%A0%E7%BB%84%E5%90%88%E6%88%90%E8%AF%8D%E8%AF%AD%E7%9A%84%E5%8F%91%E9%9F%B3%2C%20%E4%BC%A0%E7%BB%9F%E4%B8%8A%E7%94%A8%E4%B8%80%E4%BA%9Bpronunciation%20table%E3%80%82%0A%3E3.%20language%20model%E5%8D%B3%E6%A0%B9%E6%8D%AE%E5%8F%91%E9%9F%B3%E9%A2%84%E6%B5%8B%E5%AF%B9%E5%BA%94%E7%9A%84%E6%96%87%E6%9C%AC%2C%20%E4%BC%A0%E7%BB%9F%E4%B8%8A%E7%94%A8%E4%B8%80%E4%BA%9Bn-gram%20model%E3%80%82%0A%3E!%5B2fee54d58a3f6a66304abc6f9b410363.png%5D(en-resource%3A%2F%2Fdatabase%2F1155%3A0)%0A%3E%0A%3E%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%8A%80%E6%9C%AF%E5%8F%91%E5%B1%95%E5%90%8E%EF%BC%8C%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E5%8F%88%E5%8F%AF%E7%94%A8CNN%E6%9D%A5%E5%81%9A%EF%BC%8C%E5%85%B6%E4%BB%96%E9%83%A8%E5%88%86%E4%B9%9F%E5%8F%AF%E7%94%A8DNN%E6%88%96%E4%B8%80%E4%BA%9BRNN%E7%BB%93%E6%9E%84%E5%A6%82LSTM%E6%9D%A5%E5%81%9A%E3%80%82%0A%3E%E4%BD%86%E6%98%AF%E8%BF%99%E4%B8%89%E9%83%A8%E5%88%86%E6%A8%A1%E5%9E%8B%E8%BF%98%E6%98%AF%E7%9B%B8%E4%BA%92%E7%8B%AC%E7%AB%8B%E8%AE%AD%E7%BB%83%E7%9A%84%EF%BC%8C%E8%BF%99%E4%BD%BF%E5%BE%97%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B%E5%BC%82%E5%B8%B8%E5%A4%8D%E6%9D%82%EF%BC%8C%E6%88%91%E4%BB%AC%E5%B8%8C%E6%9C%9B%E8%83%BD%E7%94%A8%E4%B8%80%E4%B8%AA**end-to-end**%E7%9A%84%E6%A8%A1%E5%9E%8B%E6%9D%A5%E5%8C%85%E5%90%AB%E6%89%80%E6%9C%89%E7%9A%84%E6%AD%A5%E9%AA%A4%EF%BC%9A%E5%8F%AF%E8%BE%93%E5%85%A5%E8%AF%AD%E9%9F%B3%E6%88%96%E5%85%B6%E9%A2%91%E8%B0%B1%E8%80%8C%E7%9B%B4%E6%8E%A5%E4%BA%A7%E5%87%BA%E6%96%87%E6%9C%AC%E4%BB%8E%E8%80%8C%E7%AE%80%E5%8C%96%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B%E3%80%82%0A%3EEnd-to-end%E6%A8%A1%E5%9E%8B%EF%BC%9ACTC%EF%BC%8CRNN-T%EF%BC%8CLAS%EF%BC%8Cet.%20al.%0A%0A%0A%23%23%23%23%20%5BCTC%5D(https%3A%2F%2Fwww.cs.toronto.edu%2F~graves%2Ficml_2006.pdf)%0A%0A2006%EF%BC%8CICML%EF%BC%8CConnectionist%20Temporal%20Classification%3A%20Labelling%20Unsegmented%20Sequence%20Data%20with%20Recurrent%20Neural%20Networks%EF%BC%8C%E4%B8%BB%E8%A6%81%E6%98%AF%E4%B8%BA%E4%BA%86%E8%A7%A3%E5%86%B3%E5%88%A9%E7%94%A8LSTM%E8%AE%AD%E7%BB%83%E6%97%B6%E9%9C%80%E8%A6%81%E7%9B%AE%E6%A0%87label%E4%B8%8E%E8%BE%93%E5%85%A5%E7%9A%84%E6%AF%8F%E4%B8%80%E5%B8%A7%E9%9C%80%E8%A6%81alignment%E7%9A%84%E9%97%AE%E9%A2%98%0A%0A2009%E5%B9%B4%EF%BC%8C%E4%B8%80%E4%B8%AACTC-LSTM%20()%E7%BD%91%E7%BB%9C%E8%B5%A2%E5%BE%97%E4%BA%86%E5%A4%9A%E9%A1%B9%E8%BF%9E%E7%AC%94%E6%89%8B%E5%86%99%E8%AF%86%E5%88%AB%E7%AB%9E%E8%B5%9B%20(The%C2%A0International%20Conference%20on%20Document%20Analysis%20and%20Recognition%C2%A0(ICDAR))%EF%BC%8C%E6%88%90%E4%B8%BA%E7%AC%AC%E4%B8%80%E4%B8%AA%E8%B5%A2%E5%BE%97%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB%E7%AB%9E%E8%B5%9B%E7%9A%84RNN%E3%80%82RNN%E9%80%90%E6%B8%90%E5%BA%94%E7%94%A8%E4%BA%8Epattern%20recognition%E5%90%84%E9%A2%86%E5%9F%9F%0A%0ACTC%20is%20a%20type%20of%20neural%20network%20output%20and%20associated%20scoring%20function%2C%20for%20training%C2%A0recurrent%20neural%20networks%C2%A0(RNNs)%20such%20as%C2%A0LSTM%C2%A0networks%20to%20tackle%20sequence%20problems%20where%20the%20timing%20is%20variable.%0A%0A%0A%3ECTC%E5%AF%B9%E4%BA%8E%E8%AF%AD%E9%9F%B3%E8%AF%86%E5%88%AB%E7%9A%84%E5%A3%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E5%B8%A6%E6%9D%A5%E4%BA%86%E6%9E%81%E5%A4%A7%E7%9A%84%E5%A5%BD%E5%A4%84%0A1%EF%BC%89%E5%8C%96%E7%B9%81%E4%B8%BA%E7%AE%80%EF%BC%8C%E4%B8%8D%E5%9C%A8%E9%9C%80%E8%A6%81%E5%BC%BA%E5%88%B6%E5%AF%B9%E9%BD%90%EF%BC%8C%E5%8F%AF%E4%BB%A5%E4%BD%BF%E7%94%A8%E6%96%87%E6%9C%AC%E5%BA%8F%E5%88%97%E6%9C%AC%E8%BA%AB%E6%9D%A5%E8%BF%9B%E8%A1%8C%E5%AD%A6%E4%B9%A0%E8%AE%AD%E7%BB%83%0A2%EF%BC%89%E5%8A%A0%E9%80%9F%E8%A7%A3%E7%A0%81%EF%BC%8C%E5%A4%A7%E9%87%8FBlank%E7%9A%84%E5%AD%98%E5%9C%A8%EF%BC%8C%E4%BD%BF%E5%BE%97%E6%A8%A1%E5%9E%8B%E5%9C%A8%E8%A7%A3%E7%A0%81%E8%BF%87%E7%A8%8B%E4%B8%AD%E5%8F%AF%E4%BB%A5%E4%BD%BF%E7%94%A8%E8%B7%B3%E5%B8%A7%E6%93%8D%E4%BD%9C%EF%BC%8C%E5%9B%A0%E6%AD%A4%E5%A4%A7%E5%A4%A7%E5%8A%A0%E9%80%9F%E4%BA%86%E8%A7%A3%E7%A0%81%E8%BF%87%E7%A8%8B%E3%80%82%0A%0A%3E%E4%BD%86%E6%98%AFCTC%E6%A8%A1%E5%9E%8B%E4%BB%8D%E7%84%B6%E5%AD%98%E5%9C%A8%E7%9D%80%E5%BE%88%E5%A4%9A%E7%9A%84%E9%97%AE%E9%A2%98%EF%BC%8C%E5%85%B6%E4%B8%AD%E6%9C%80%E6%98%BE%E8%91%97%E7%9A%84%E5%B0%B1%E6%98%AFCTC%E5%81%87%E8%AE%BE%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%BE%93%E5%87%BA%E4%B9%8B%E9%97%B4%E6%98%AF%E6%9D%A1%E4%BB%B6%E7%8B%AC%E7%AB%8B%E7%9A%84%E3%80%82%E8%BF%99%E4%B8%AA%E5%9F%BA%E6%9C%AC%E5%81%87%E8%AE%BE%E4%B8%8E%E8%AF%AD%E9%9F%B3%E8%AF%86%E5%88%AB%E4%BB%BB%E5%8A%A1%E4%B9%8B%E5%89%8D%E5%AD%98%E5%9C%A8%E7%9D%80%E4%B8%80%E5%AE%9A%E7%A8%8B%E5%BA%A6%E7%9A%84%E8%83%8C%E7%A6%BB%E3%80%82%E6%AD%A4%E5%A4%96%EF%BC%8CCTC%E6%A8%A1%E5%9E%8B%E5%B9%B6%E4%B8%8D%E5%85%B7%E6%9C%89%E8%AF%AD%E8%A8%80%E5%BB%BA%E6%A8%A1%E8%83%BD%E5%8A%9B%EF%BC%8C%E5%90%8C%E6%97%B6%E4%B9%9F%E5%B9%B6%E6%B2%A1%E6%9C%89%E7%9C%9F%E6%AD%A3%E7%9A%84%E5%AE%9E%E7%8E%B0%E7%AB%AF%E5%88%B0%E7%AB%AF%E7%9A%84%E8%81%94%E5%90%88%E4%BC%98%E5%8C%96%E3%80%82%0A%0A%23%23%23%23%20RNN-T%0A%E5%9F%BA%E4%BA%8ECTC%E7%9A%84%E6%94%B9%E8%BF%9B%EF%BC%8C%E8%AF%AD%E9%9F%B3%E8%AF%86%E5%88%AB%2C%0A2012%EF%BC%8C%5BSequence%20Transduction%20with%20Recurrent%20Neural%20Network%5D(https%3A%2F%2Farxiv.org%2Fabs%2F1211.3711)%0A2014%EF%BC%8C%5BSequence%20to%20Sequence%20Learning%20with%20Neural%20Networks%5D(https%3A%2F%2Farxiv.org%2Fabs%2F1409.3215)%0A%0A%0A%E9%92%88%E5%AF%B9CTC%E7%9A%84%E4%B8%8D%E8%B6%B3%EF%BC%8CAlex%20Graves%E5%9C%A82012%E5%B9%B4%E5%B7%A6%E5%8F%B3%E6%8F%90%E5%87%BA%E4%BA%86RNN-T%E6%A8%A1%E5%9E%8B%EF%BC%8CRNN-T%E6%A8%A1%E5%9E%8B%E5%B7%A7%E5%A6%99%E7%9A%84%E5%B0%86%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%A3%B0%E5%AD%A6%E6%A8%A1%E5%9E%8B%E6%95%B4%E5%90%88%E5%9C%A8%E4%B8%80%E8%B5%B7%EF%BC%8C%E5%90%8C%E6%97%B6%E8%BF%9B%E8%A1%8C%E8%81%94%E5%90%88%E4%BC%98%E5%8C%96%EF%BC%8C%E6%98%AF%E4%B8%80%E7%A7%8D%E7%90%86%E8%AE%BA%E4%B8%8A%E7%9B%B8%E5%AF%B9%E5%AE%8C%E7%BE%8E%E7%9A%84%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84%E3%80%82%0ARNN-T%E6%A8%A1%E5%9E%8B%E5%BC%95%E5%85%A5%E4%BA%86TranscriptionNet%E4%B9%9F%E5%B0%B1%E6%98%AF%E5%9B%BE%E4%B8%AD%E7%9A%84Encoder%EF%BC%88%E5%8F%AF%E4%BB%A5%E4%BD%BF%E7%94%A8%E4%BB%BB%E4%BD%95%E5%A3%B0%E5%AD%A6%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%BB%93%E6%9E%84%EF%BC%89%EF%BC%8C%E7%9B%B8%E5%BD%93%E4%BA%8E%E5%A3%B0%E5%AD%A6%E6%A8%A1%E5%9E%8B%E9%83%A8%E5%88%86%EF%BC%8C%E5%9B%BE%E4%B8%AD%E7%9A%84PredictionNet%E5%AE%9E%E9%99%85%E4%B8%8A%E7%9B%B8%E5%BD%93%E4%BA%8E%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%88%E5%8F%AF%E4%BB%A5%E4%BD%BF%E7%94%A8%E5%8D%95%E5%90%91%E7%9A%84%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9D%A5%E6%9E%84%E5%BB%BA%EF%BC%89%E3%80%82%0A%E6%A8%A1%E5%9E%8B%E4%B8%AD%E6%AF%94%E8%BE%83%E6%96%B0%E5%A5%87%EF%BC%8C%E5%90%8C%E6%97%B6%E4%B9%9F%E6%98%AF%E6%9C%80%E9%87%8D%E8%A6%81%E7%9A%84%E7%BB%93%E6%9E%84%E5%B0%B1%E6%98%AF%E8%81%94%E5%90%88%E7%BD%91%E7%BB%9CJoint%20Net%EF%BC%8C%E4%B8%80%E8%88%AC%E5%8F%AF%E4%BB%A5%E4%BD%BF%E7%94%A8%E5%89%8D%E5%90%91%E7%BD%91%E7%BB%9C%E6%9D%A5%E8%BF%9B%E8%A1%8C%E5%BB%BA%E6%A8%A1%E3%80%82%0A%E8%81%94%E5%90%88%E7%BD%91%E7%BB%9C%E7%9A%84%E4%BD%9C%E7%94%A8%E5%B0%B1%E6%98%AF%E5%B0%86%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%92%8C%E5%A3%B0%E5%AD%A6%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%8A%B6%E6%80%81%E9%80%9A%E8%BF%87%E6%9F%90%E7%A7%8D%E6%80%9D%E8%B7%AF%E7%BB%93%E5%90%88%E5%9C%A8%E4%B8%80%E8%B5%B7%EF%BC%8C%E5%8F%AF%E4%BB%A5%E6%98%AF%E6%8B%BC%E6%8E%A5%E6%93%8D%E4%BD%9C%EF%BC%8C%E4%B9%9F%E5%8F%AF%E4%BB%A5%E6%98%AF%E7%9B%B4%E6%8E%A5%E7%9B%B8%E5%8A%A0%E7%AD%89%EF%BC%8C%E8%80%83%E8%99%91%E5%88%B0%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%92%8C%E5%A3%B0%E5%AD%A6%E6%A8%A1%E5%9E%8B%E5%8F%AF%E8%83%BD%E6%9C%89%E4%B8%8D%E5%90%8C%E7%9A%84%E6%9D%83%E9%87%8D%E9%97%AE%E9%A2%98%EF%BC%8C%E4%BC%BC%E4%B9%8E%E6%8B%BC%E6%8E%A5%E6%93%8D%E4%BD%9C%E6%9B%B4%E5%8A%A0%E5%90%88%E7%90%86%E4%B8%80%E4%BA%9B%E3%80%82%0A%0A%23%23%23%23%20%5BGRU%5D(https%3A%2F%2Farxiv.org%2Fabs%2F1406.1078)%0A2014%EF%BC%8Ca%C2%A0long%20short-term%20memory%C2%A0(LSTM)%20with%20a%20forget%20gate%0A%0A%E7%9B%B8%E6%AF%94LSTM%EF%BC%8C%E4%BD%BF%E7%94%A8GRU%E8%83%BD%E5%A4%9F%E8%BE%BE%E5%88%B0%E7%9B%B8%E5%BD%93%E7%9A%84%E6%95%88%E6%9E%9C%EF%BC%8C%E5%B9%B6%E4%B8%94%E7%9B%B8%E6%AF%94%E4%B9%8B%E4%B8%8B%E6%9B%B4%E5%AE%B9%E6%98%93%E8%BF%9B%E8%A1%8C%E8%AE%AD%E7%BB%83%EF%BC%8C%E8%83%BD%E5%A4%9F%E5%BE%88%E5%A4%A7%E7%A8%8B%E5%BA%A6%E4%B8%8A%E6%8F%90%E9%AB%98%E8%AE%AD%E7%BB%83%E6%95%88%E7%8E%87%0A!%5B58724f49f512d7d56f93057fbd9d5490.png%5D(en-resource%3A%2F%2Fdatabase%2F1157%3A0)%0A%0A%0A%23%23%23%23%20%5BRecurrent%20convolutional%20NN%5D(https%3A%2F%2Faaai.org%2Focs%2Findex.php%2FAAAI%2FAAAI15%2Fpaper%2Fview%2F9745)%0A2015%20AAAI%EF%BC%8CRecurrent%20Convolutional%20Neural%20Networks%20for%20Text%20Classification%0A%0A%0A%E6%9C%AC%E6%96%87%E8%A6%81%E8%A7%A3%E5%86%B3%E7%9A%84%E9%97%AE%E9%A2%98%E6%98%AF%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%EF%BC%8C%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%9C%80%E5%85%B3%E9%94%AE%E7%9A%84%E9%97%AE%E9%A2%98%E6%98%AF%E7%89%B9%E5%BE%81%E8%A1%A8%E7%A4%BA%EF%BC%8C%E4%BC%A0%E7%BB%9F%E7%9A%84%E6%96%B9%E6%B3%95%E7%BB%8F%E5%B8%B8%E4%BC%9A%E5%BF%BD%E7%95%A5%E4%B8%8A%E4%B8%8B%E6%96%87%E4%BF%A1%E6%81%AF%E5%92%8C%E8%AF%8D%E5%BA%8F%EF%BC%8C%E6%97%A0%E6%B3%95%E6%8D%95%E6%8D%89%E5%88%B0%E8%AF%8D%E4%B9%89%E3%80%82%0A%0A%E4%B8%BA%E4%BA%86%E8%A7%A3%E5%86%B3RNN%E7%9A%84%E6%9C%89%E5%81%8F%E6%80%A7%E9%97%AE%E9%A2%98%EF%BC%8C%E6%9C%89%E7%9A%84%E7%A0%94%E7%A9%B6%E8%80%85%E6%8F%90%E5%87%BA%E4%BA%86%E7%94%A8CNN%EF%BC%88%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%89%E6%9D%A5%E8%A1%A8%E7%A4%BA%E6%96%87%E6%9C%AC%EF%BC%8C%E5%B9%B6%E4%B8%94%E6%97%B6%E9%97%B4%E5%A4%8D%E6%9D%82%E5%BA%A6%E4%B9%9F%E6%98%AFO(n)%EF%BC%8C%E4%BD%86%E6%98%AFCNN%E5%AD%98%E5%9C%A8%E4%B8%80%E4%B8%AA%E7%BC%BA%E9%99%B7%EF%BC%8C%E5%8D%B7%E7%A7%AF%E7%AA%97%E5%8F%A3%E7%9A%84%E5%A4%A7%E5%B0%8F%E6%98%AF%E5%9B%BA%E5%AE%9A%E7%9A%84%EF%BC%8C%E5%B9%B6%E4%B8%94%E8%BF%99%E4%B8%AA%E7%AA%97%E5%8F%A3%E5%A4%A7%E5%B0%8F%E5%A6%82%E4%BD%95%E8%AE%BE%E7%BD%AE%E6%98%AF%E4%B8%80%E4%B8%AA%E9%97%AE%E9%A2%98%EF%BC%8C%E5%A6%82%E6%9E%9C%E8%AE%BE%E7%BD%AE%E5%B0%8F%E4%BA%86%EF%BC%8C%E5%88%99%E4%BC%9A%E6%8D%9F%E5%A4%B1%E6%9C%89%E6%95%88%E4%BF%A1%E6%81%AF%EF%BC%8C%E5%A6%82%E6%9E%9C%E8%AE%BE%E7%BD%AE%E5%A4%A7%E4%BA%86%EF%BC%8C%E4%BC%9A%E5%A2%9E%E5%8A%A0%E5%BE%88%E5%A4%9A%E7%9A%84%E5%8F%82%E6%95%B0%E3%80%82%0A%E4%BA%8E%E6%98%AF%EF%BC%8C%E9%92%88%E5%AF%B9%E4%B8%8A%E8%BF%B0%E6%A8%A1%E5%9E%8B%E5%AD%98%E5%9C%A8%E7%9A%84%E9%97%AE%E9%A2%98%EF%BC%8C%E6%9C%AC%E6%96%87%E6%8F%90%E5%87%BA%E4%BA%86RCNN%EF%BC%88%E5%BE%AA%E7%8E%AF%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%89%E6%A8%A1%E5%9E%8B%0A!%5B1912815463e812d2aa0f35a9db77e363.png%5D(en-resource%3A%2F%2Fdatabase%2F1159%3A0)%0A%0A%0A%23%23%23%23%20%5BBERT%5D(https%3A%2F%2Farxiv.org%2Fabs%2F1810.04805)%0A2019%EF%BC%8CBERT%3A%20Pre-training%20of%20Deep%20Bidirectional%20Transformers%20for%20Language%20Understanding%EF%BC%8C%E8%AF%AD%E4%B9%89%E7%90%86%E8%A7%A3%0A%0A%0ABERT%EF%BC%88Bidirectional%C2%A0Encoder%C2%A0Representations%20from%C2%A0Transformers%EF%BC%89%E8%BF%91%E6%9C%9F%E6%8F%90%E5%87%BA%E4%B9%8B%E5%90%8E%EF%BC%8C%E4%BD%9C%E4%B8%BA%E4%B8%80%E4%B8%AAWord2Vec%E7%9A%84%E6%9B%BF%E4%BB%A3%E8%80%85%EF%BC%8C%E5%85%B6%E5%9C%A8NLP%E9%A2%86%E5%9F%9F%E7%9A%8411%E4%B8%AA%E6%96%B9%E5%90%91%E5%A4%A7%E5%B9%85%E5%88%B7%E6%96%B0%E4%BA%86%E7%B2%BE%E5%BA%A6%EF%BC%8C%E5%8F%AF%E4%BB%A5%E8%AF%B4%E6%98%AF%E8%BF%91%E5%B9%B4%E6%9D%A5%E8%87%AA%E6%AE%8B%E5%B7%AE%E7%BD%91%E7%BB%9C%E6%9C%80%E4%BC%98%E7%AA%81%E7%A0%B4%E6%80%A7%E7%9A%84%E4%B8%80%E9%A1%B9%E6%8A%80%E6%9C%AF%E4%BA%86%0A%0A1.%20%E4%BD%BF%E7%94%A8%E4%BA%86Transformer%20%5B2%5D%E4%BD%9C%E4%B8%BA%E7%AE%97%E6%B3%95%E7%9A%84%E4%B8%BB%E8%A6%81%E6%A1%86%E6%9E%B6%EF%BC%8CTransformer%E8%83%BD%E6%9B%B4%E5%BD%BB%E5%BA%95%E7%9A%84%E6%8D%95%E6%8D%89%E8%AF%AD%E5%8F%A5%E4%B8%AD%E7%9A%84%E5%8F%8C%E5%90%91%E5%85%B3%E7%B3%BB%EF%BC%9B%0A2.%20%E4%BD%BF%E7%94%A8%E4%BA%86Mask%20Language%20Model(MLM)%20%5B3%5D%20%E5%92%8C%20Next%20Sentence%20Prediction(NSP)%20%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E8%AE%AD%E7%BB%83%E7%9B%AE%E6%A0%87%EF%BC%9B%0A3.%20%E4%BD%BF%E7%94%A8%E6%9B%B4%E5%BC%BA%E5%A4%A7%E7%9A%84%E6%9C%BA%E5%99%A8%E8%AE%AD%E7%BB%83%E6%9B%B4%E5%A4%A7%E8%A7%84%E6%A8%A1%E7%9A%84%E6%95%B0%E6%8D%AE%EF%BC%8C%E4%BD%BFBERT%E7%9A%84%E7%BB%93%E6%9E%9C%E8%BE%BE%E5%88%B0%E4%BA%86%E5%85%A8%E6%96%B0%E7%9A%84%E9%AB%98%E5%BA%A6%EF%BC%8C%E5%B9%B6%E4%B8%94Google%E5%BC%80%E6%BA%90%E4%BA%86BERT%E6%A8%A1%E5%9E%8B%EF%BC%8C%E7%94%A8%E6%88%B7%E5%8F%AF%E4%BB%A5%E7%9B%B4%E6%8E%A5%E4%BD%BF%E7%94%A8BERT%E4%BD%9C%E4%B8%BAWord2Vec%E7%9A%84%E8%BD%AC%E6%8D%A2%E7%9F%A9%E9%98%B5%E5%B9%B6%E9%AB%98%E6%95%88%E7%9A%84%E5%B0%86%E5%85%B6%E5%BA%94%E7%94%A8%E5%88%B0%E8%87%AA%E5%B7%B1%E7%9A%84%E4%BB%BB%E5%8A%A1%E4%B8%AD%E3%80%82%0A%0A%0ABERT%E7%9A%84%E6%9C%AC%E8%B4%A8%E4%B8%8A%E6%98%AF%E9%80%9A%E8%BF%87%E5%9C%A8%E6%B5%B7%E9%87%8F%E7%9A%84%E8%AF%AD%E6%96%99%E7%9A%84%E5%9F%BA%E7%A1%80%E4%B8%8A%E8%BF%90%E8%A1%8C%E8%87%AA%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E4%B8%BA%E5%8D%95%E8%AF%8D%E5%AD%A6%E4%B9%A0%E4%B8%80%E4%B8%AA%E5%A5%BD%E7%9A%84%E7%89%B9%E5%BE%81%E8%A1%A8%E7%A4%BA%EF%BC%8C%E6%89%80%E8%B0%93%E8%87%AA%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E6%98%AF%E6%8C%87%E5%9C%A8%E6%B2%A1%E6%9C%89%E4%BA%BA%E5%B7%A5%E6%A0%87%E6%B3%A8%E7%9A%84%E6%95%B0%E6%8D%AE%E4%B8%8A%E8%BF%90%E8%A1%8C%E7%9A%84%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%82%0A%E5%9C%A8%E4%BB%A5%E5%90%8E%E7%89%B9%E5%AE%9A%E7%9A%84NLP%E4%BB%BB%E5%8A%A1%E4%B8%AD%EF%BC%8C%E6%88%91%E4%BB%AC%E5%8F%AF%E4%BB%A5%E7%9B%B4%E6%8E%A5%E4%BD%BF%E7%94%A8BERT%E7%9A%84%E7%89%B9%E5%BE%81%E8%A1%A8%E7%A4%BA%E4%BD%9C%E4%B8%BA%E8%AF%A5%E4%BB%BB%E5%8A%A1%E7%9A%84%E8%AF%8D%E5%B5%8C%E5%85%A5%E7%89%B9%E5%BE%81%E3%80%82%E6%89%80%E4%BB%A5BERT%E6%8F%90%E4%BE%9B%E7%9A%84%E6%98%AF%E4%B8%80%E4%B8%AA%E4%BE%9B%E5%85%B6%E5%AE%83%E4%BB%BB%E5%8A%A1%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%A8%A1%E5%9E%8B%EF%BC%8C%E8%AF%A5%E6%A8%A1%E5%9E%8B%E5%8F%AF%E4%BB%A5%E6%A0%B9%E6%8D%AE%E4%BB%BB%E5%8A%A1%E5%BE%AE%E8%B0%83%E6%88%96%E8%80%85%E5%9B%BA%E5%AE%9A%E4%B9%8B%E5%90%8E%E4%BD%9C%E4%B8%BA%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E5%99%A8%E3%80%82%0A%0A%23%23%23%23%20%5BGPT-3%5D(https%3A%2F%2Farxiv.org%2Fabs%2F2005.14165)%0Ahttps%3A%2F%2Fgithub.com%2Fopenai%2Fgpt-3%0A2020%EF%BC%8C%0ALanguage%20Models%20are%20Few-Shot%20Learners%0A%0A%E5%85%B7%E6%9C%891%2C750%E4%BA%BF%E4%B8%AA%E5%8F%82%E6%95%B0%E7%9A%84%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%EF%BC%8C%E6%AF%94%E4%BB%A5%E5%89%8D%E7%9A%84%E7%89%88%E6%9C%ACGPT-2%E9%AB%98100%E5%80%8D%0A%E8%AF%A5%E6%A8%A1%E5%9E%8B%E7%BB%8F%E8%BF%87%E4%BA%86%E5%B0%86%E8%BF%910.5%E4%B8%87%E4%BA%BF%E4%B8%AA%E5%8D%95%E8%AF%8D%E7%9A%84%E9%A2%84%E8%AE%AD%E7%BB%83%EF%BC%8C%E5%B9%B6%E4%B8%94%E5%9C%A8%E4%B8%8D%E8%BF%9B%E8%A1%8C%E5%BE%AE%E8%B0%83%E7%9A%84%E6%83%85%E5%86%B5%E4%B8%8B%EF%BC%8C%E5%8F%AF%E4%BB%A5%E5%9C%A8%E5%A4%9A%E4%B8%AANLP%E5%9F%BA%E5%87%86%E4%B8%8A%E8%BE%BE%E5%88%B0%E6%9C%80%E5%85%88%E8%BF%9B%E7%9A%84%E6%80%A7%E8%83%BD%E3%80%82%0A%0A%0AGPT-3%20%E6%9C%80%E4%BB%A4%E4%BA%BA%E6%83%8A%E8%AE%B6%E7%9A%84%E8%BF%98%E6%98%AF%E6%A8%A1%E5%9E%8B%E4%BD%93%E9%87%8F%EF%BC%8C%E5%AE%83%E4%BD%BF%E7%94%A8%E7%9A%84%E6%9C%80%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9B%86%E5%9C%A8%E5%A4%84%E7%90%86%E5%89%8D%E5%AE%B9%E9%87%8F%E8%BE%BE%E5%88%B0%E4%BA%86%2045TB%E3%80%82</center></span>
</div></body></html> 