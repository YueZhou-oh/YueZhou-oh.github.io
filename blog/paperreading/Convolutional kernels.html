<html>
<head>
  <title>Evernote Export</title>
  <basefont face="微软雅黑" size="2" />
  <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
  <meta name="exporter-version" content="YXBJ Windows/602628 (zh-CN, DDL); Windows/10.0.0 (Win64); EDAMVersion=V2;"/>
  <meta name="content-class" content="yinxiang.markdown"/>
  <style>
    body, td {
      font-family: 微软雅黑;
      font-size: 10pt;
    }
  </style>
</head>
<body>
<a name="879"/>

<div><span><div style="font-size: 14px; margin: 0; padding: 0; width: 100%;"><div style="line-height: 160%; box-sizing: content-box;"><ul style="line-height: 160%; box-sizing: content-box; display: block; list-style-type: disc; padding-left: 30px; margin: 6px 0 10px; color: #333;"><li style="line-height: 160%; box-sizing: content-box; position: relative;"><a style="line-height: 160%; box-sizing: content-box; text-decoration: underline; color: #5286bc;">Convolutions
</a><ul style="line-height: 160%; box-sizing: content-box; display: block; list-style-type: disc; padding-left: 30px; margin: 6px 0 10px; color: #333; margin-top: 0; margin-bottom: 0;"><li style="line-height: 160%; box-sizing: content-box; position: relative;"><a style="line-height: 160%; box-sizing: content-box; text-decoration: underline; color: #5286bc;">1 Group convolution
</a></li><li style="line-height: 160%; box-sizing: content-box; position: relative;"><a style="line-height: 160%; box-sizing: content-box; text-decoration: underline; color: #5286bc;">2 Depthwise (DW)/Pointwise convolution (PW)
</a></li><li style="line-height: 160%; box-sizing: content-box; position: relative;"><a style="line-height: 160%; box-sizing: content-box; text-decoration: underline; color: #5286bc;">3 Inverted residual block &amp;&amp; Linear Bottlenecks
</a></li><li style="line-height: 160%; box-sizing: content-box; position: relative;"><a style="line-height: 160%; box-sizing: content-box; text-decoration: underline; color: #5286bc;">4 Fire module
</a></li><li style="line-height: 160%; box-sizing: content-box; position: relative;"><a style="line-height: 160%; box-sizing: content-box; text-decoration: underline; color: #5286bc;">5 Dilated/Atrous convolution
</a></li><li style="line-height: 160%; box-sizing: content-box; position: relative;"><a style="line-height: 160%; box-sizing: content-box; text-decoration: underline; color: #5286bc;">6 Fractionally-strided convolution
</a></li><li style="line-height: 160%; box-sizing: content-box; position: relative;"><a style="line-height: 160%; box-sizing: content-box; text-decoration: underline; color: #5286bc;">7 project and reshape
</a></li><li style="line-height: 160%; box-sizing: content-box; position: relative;"><a style="line-height: 160%; box-sizing: content-box; text-decoration: underline; color: #5286bc;">8 Shift Conv
</a></li></ul></li><li style="line-height: 160%; box-sizing: content-box; position: relative;"><a style="line-height: 160%; box-sizing: content-box; text-decoration: underline; color: #5286bc;">Deconvolutions
</a><ul style="line-height: 160%; box-sizing: content-box; display: block; list-style-type: disc; padding-left: 30px; margin: 6px 0 10px; color: #333; margin-top: 0; margin-bottom: 0;"><li style="line-height: 160%; box-sizing: content-box; position: relative;"><a style="line-height: 160%; box-sizing: content-box; text-decoration: underline; color: #5286bc;">1. Upsample
</a></li><li style="line-height: 160%; box-sizing: content-box; position: relative;"><a style="line-height: 160%; box-sizing: content-box; text-decoration: underline; color: #5286bc;">2. Transposed convolution
</a></li><li style="line-height: 160%; box-sizing: content-box; position: relative;"><a style="line-height: 160%; box-sizing: content-box; text-decoration: underline; color: #5286bc;">3. keras.layers.Upsample2D
</a></li><li style="line-height: 160%; box-sizing: content-box; position: relative;"><a style="line-height: 160%; box-sizing: content-box; text-decoration: underline; color: #5286bc;">4. tensorflow.nn.conv2d_transpose
</a></li></ul></li></ul></div><h3 style="line-height: 160%; box-sizing: content-box; font-weight: 700; font-size: 27px; color: #333;">Convolutions</h3>
<h4 style="line-height: 160%; box-sizing: content-box; font-size: 20px; color: #333;">1 Group convolution</h4>
<blockquote style="line-height: 160%; box-sizing: content-box; margin: 15px 0; border-left: 4px solid #ddd; padding: 0 15px; color: #777;">
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333; margin-top: 0; margin-bottom: 0;">最早见于AlexNet，用来切分网络在两个GPU上训练，推广: <a href="https://arxiv.org/abs/1707.01083" style="line-height: 160%; box-sizing: content-box; text-decoration: underline; color: #5286bc;">ShuffleNet</a>, extremely efficient for mobile devices</p>
</blockquote>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;">AlexNet：<br/>
<img src="Convolutional kernels_files/Image.png" type="image/png" data-filename="Image.png"/><br/>
Group conv:<br/>
<img src="Convolutional kernels_files/Image [1].png" type="image/png" data-filename="Image.png"/></p>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;"><strong style="line-height: 160%; box-sizing: content-box; font-weight: 700;">优势：</strong></p>
<ul style="line-height: 160%; box-sizing: content-box; display: block; list-style-type: disc; padding-left: 30px; margin: 6px 0 10px; color: #333;">
<li style="line-height: 160%; box-sizing: content-box; position: relative;">减少参数量，减少计算量</li>
<li style="line-height: 160%; box-sizing: content-box; position: relative;">Group convolution可以看成是structure sparse，相当于正则，在减少参数量的同时获得更好的效果</li>
<li style="line-height: 160%; box-sizing: content-box; position: relative;">当分组数等于输入map数量时，等价于depthwise convolution，参数量进一步减少</li>
<li style="line-height: 160%; box-sizing: content-box; position: relative;">其它情况可等价于GAP等<br/>
<img src="Convolutional kernels_files/Image [2].png" type="image/png" data-filename="Image.png"/><br/>
<img src="Convolutional kernels_files/Image [3].png" type="image/png" data-filename="Image.png"/></li>
</ul>
<h4 style="line-height: 160%; box-sizing: content-box; font-size: 20px; color: #333;">2 Depthwise (DW)/Pointwise convolution (PW)</h4>
<blockquote style="line-height: 160%; box-sizing: content-box; margin: 15px 0; border-left: 4px solid #ddd; padding: 0 15px; color: #777;">
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333; margin-top: 0; margin-bottom: 0;">提出: <a href="http://arxiv.org/abs/1704.04861" style="line-height: 160%; box-sizing: content-box; text-decoration: underline; color: #5286bc;">MobileNet</a>, 提高训练速度与效率，speed&amp;accuracy&amp;efficiency</p>
</blockquote>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;">Depthwise Separable Convolution是将一个完整的卷积运算分解为两步进行，即Depthwise Convolution与Pointwise Convolution。</p>
<ul style="line-height: 160%; box-sizing: content-box; display: block; list-style-type: disc; padding-left: 30px; margin: 6px 0 10px; color: #333;">
<li style="line-height: 160%; box-sizing: content-box; position: relative;">
<p style="line-height: 160%; box-sizing: content-box; color: #333; margin: 0;"><strong style="line-height: 160%; box-sizing: content-box; font-weight: 700;">Depthwise convolution:</strong><br/>
Depthwise Convolution的一个卷积核负责一个通道，一个通道只被一个卷积核卷积。<br/>
<img src="Convolutional kernels_files/Image [4].png" type="image/png" data-filename="Image.png"/></p>
</li>
<li style="line-height: 160%; box-sizing: content-box; position: relative;">
<p style="line-height: 160%; box-sizing: content-box; color: #333; margin: 0;"><strong style="line-height: 160%; box-sizing: content-box; font-weight: 700;">Pointwise convolution</strong><br/>
Pointwise Convolution的运算<strong style="line-height: 160%; box-sizing: content-box; font-weight: 700;">与常规卷积运算非常相似(kernel size的区别)</strong> ，它的卷积核的尺寸为 1×1×M，M为上一层的通道数。所以这里的卷积运算会将上一步的map在深度方向上进行加权组合，生成新的Feature map。有几个卷积核就有几个输出Feature map。<br/>
<img src="Convolutional kernels_files/Image [5].png" type="image/png" data-filename="Image.png"/></p>
</li>
</ul>
<blockquote style="line-height: 160%; box-sizing: content-box; margin: 15px 0; border-left: 4px solid #ddd; padding: 0 15px; color: #777;">
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333; margin-top: 0; margin-bottom: 0;">回顾一下，常规卷积的<strong style="line-height: 160%; box-sizing: content-box; font-weight: 700;">参数个数</strong>为：<br/>
N_std = 4 × 3 × 3 × 3 = 108<br/>
Separable Convolution的参数由两部分相加得到：<br/>
N_depthwise = 3 × 3 × 3 = 27<br/>
N_pointwise = 1 × 1 × 3 × 4 = 12<br/>
N_separable = N_depthwise + N_pointwise = 39<br/>
相同的输入，同样是得到4张Feature map，Separable Convolution的参数个数是常规卷积的约1/3。因此，<strong style="line-height: 160%; box-sizing: content-box; font-weight: 700;">在参数量相同的前提下，采用Separable Convolution的神经网络层数可以做的更深。</strong></p>
</blockquote>
<h4 style="line-height: 160%; box-sizing: content-box; font-size: 20px; color: #333;">3 Inverted residual block &amp;&amp; Linear Bottlenecks</h4>
<blockquote style="line-height: 160%; box-sizing: content-box; margin: 15px 0; border-left: 4px solid #ddd; padding: 0 15px; color: #777;">
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333; margin-top: 0; margin-bottom: 0;">提出: <a href="https://arxiv.org/abs/1801.04381" style="line-height: 160%; box-sizing: content-box; text-decoration: underline; color: #5286bc;">MobleNet-V2</a>，improvement in terms of accuracy and speed</p>
</blockquote>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;"><img src="Convolutional kernels_files/Image [6].png" type="image/png" data-filename="Image.png"/></p>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;"><strong style="line-height: 160%; box-sizing: content-box; font-weight: 700;">传统的residual block</strong>，先用1x1卷积将输入的feature map的<strong style="line-height: 160%; box-sizing: content-box; font-weight: 700;">维度降低</strong>，然后进行3x3的卷积操作，最后再用1x1的卷积将<strong style="line-height: 160%; box-sizing: content-box; font-weight: 700;">维度变大</strong>。<br/>
<strong style="line-height: 160%; box-sizing: content-box; font-weight: 700;">Inverted residual block</strong>先用1x1卷积将输入的feature map<strong style="line-height: 160%; box-sizing: content-box; font-weight: 700;">维度变大</strong>，然后用3x3 <strong style="line-height: 160%; box-sizing: content-box; font-weight: 700;">depthwise convolution</strong>方式做卷积运算，最后使用1x1的卷积运算将其<strong style="line-height: 160%; box-sizing: content-box; font-weight: 700;">维度缩小</strong>。注意，此时的1x1卷积运算后，<strong style="line-height: 160%; box-sizing: content-box; font-weight: 700;">不再使用ReLU激活函数</strong>，而是使用<strong style="line-height: 160%; box-sizing: content-box; font-weight: 700;">线性激活函数 (linear bottleneck)</strong>，以保留更多特征信息，保证模型的表达能力。</p>
<h4 style="line-height: 160%; box-sizing: content-box; font-size: 20px; color: #333;">4 Fire module</h4>
<blockquote style="line-height: 160%; box-sizing: content-box; margin: 15px 0; border-left: 4px solid #ddd; padding: 0 15px; color: #777;">
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333; margin-top: 0; margin-bottom: 0;">提出: <a href="https://arxiv.org/abs/1602.07360" style="line-height: 160%; box-sizing: content-box; text-decoration: underline; color: #5286bc;">Squeezenet</a>, 实现模型压缩，减少参数量50x</p>
</blockquote>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;"><img src="Convolutional kernels_files/Image [7].png" type="image/png" data-filename="Image.png"/></p>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;">在Fire module中作者使用了<strong style="line-height: 160%; box-sizing: content-box; font-weight: 700;">三个hyper parameters</strong>用于表示它的构成。<br/>
s1x1表示<strong style="line-height: 160%; box-sizing: content-box; font-weight: 700;">squeeze layer filters</strong>的数目；e1x1表示<strong style="line-height: 160%; box-sizing: content-box; font-weight: 700;">expand layer</strong>中1x1 conv filters的数目，e3x3则表示expand layer中3x3 conv filters的数目。<br/>
因为在每个fire module内部s1x1要远小于e1x1 + e3x3，它们满足s1x1 = SR * (e1x1 + e3x3)。而<strong style="line-height: 160%; box-sizing: content-box; font-weight: 700;">SR称为缩减系数</strong>，在这里只有0.125。</p>
<h4 style="line-height: 160%; box-sizing: content-box; font-size: 20px; color: #333;">5 Dilated/Atrous convolution</h4>
<blockquote style="line-height: 160%; box-sizing: content-box; margin: 15px 0; border-left: 4px solid #ddd; padding: 0 15px; color: #777;">
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333; margin-top: 0; margin-bottom: 0;">提出: <a href="https://arxiv.org/abs/1511.07122" style="line-height: 160%; box-sizing: content-box; text-decoration: underline; color: #5286bc;">Multi-Scale Context Aggregation by Dilated Convolutions</a>, 常用于语义分割<br/>
VGG中的发现： 7 x 7 的卷积层的正则等效于 3 个 3 x 3 的卷积层的叠加，这样的设计不仅可以大幅度的<strong style="line-height: 160%; box-sizing: content-box; font-weight: 700;">减少参数</strong>，其本身带有<strong style="line-height: 160%; box-sizing: content-box; font-weight: 700;">正则性质</strong>的 convolution map 能够更容易学一个 generlisable, expressive feature space。这也是现在绝大部分基于卷积的<strong style="line-height: 160%; box-sizing: content-box; font-weight: 700;">深层网络</strong>都在用<strong style="line-height: 160%; box-sizing: content-box; font-weight: 700;">小卷积核</strong>的原因。<br/>
<img src="Convolutional kernels_files/Image [8].png" type="image/png" data-filename="Image.png"/></p>
</blockquote>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;">然而 Deep CNN 对于其他任务还有一些致命性的缺陷。较为著名的是 up-sampling 和 pooling layer 的设计。这个在 Hinton 的演讲里也一直提到过。主要问题有：</p>
<ul style="line-height: 160%; box-sizing: content-box; display: block; list-style-type: disc; padding-left: 30px; margin: 6px 0 10px; color: #333;">
<li style="line-height: 160%; box-sizing: content-box; position: relative;">Up-sampling / pooling layer (e.g. bilinear interpolation) is deterministic. (a.k.a. not learnable)</li>
<li style="line-height: 160%; box-sizing: content-box; position: relative;">内部数据结构丢失；空间层级化信息丢失。</li>
<li style="line-height: 160%; box-sizing: content-box; position: relative;"><strong style="line-height: 160%; box-sizing: content-box; font-weight: 700;">小物体信息无法重建</strong> (假设有四个pooling layer 则 任何小于 2^4 = 16 pixel 的物体信息将理论上无法重建。)</li>
</ul>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;">在这样问题的存在下，<strong style="line-height: 160%; box-sizing: content-box; font-weight: 700;">语义分割问题</strong>一直处在瓶颈期无法再明显提高精度， 而 dilated convolution 的设计就良好的避免了这些问题。</p>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;"><strong style="line-height: 160%; box-sizing: content-box; font-weight: 700;">Dilated convolution优点：</strong><br/>
内部数据结构的保留和避免使用 down-sampling 这样的特性。</p>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;"><img src="Convolutional kernels_files/Image [9].png" type="image/png" data-filename="Image.png"/></p>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;"><strong style="line-height: 160%; box-sizing: content-box; font-weight: 700;">缺陷：</strong></p>
<ul style="line-height: 160%; box-sizing: content-box; display: block; list-style-type: disc; padding-left: 30px; margin: 6px 0 10px; color: #333;">
<li style="line-height: 160%; box-sizing: content-box; position: relative;">The gridding effect, related to dilation rate</li>
<li style="line-height: 160%; box-sizing: content-box; position: relative;">Long-ranged information might be not relevent</li>
</ul>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;">图森组的文章的solution：<strong style="line-height: 160%; box-sizing: content-box; font-weight: 700;">Hybrid Dilated convolution (HDC)</strong></p>
<ul style="line-height: 160%; box-sizing: content-box; display: block; list-style-type: disc; padding-left: 30px; margin: 6px 0 10px; color: #333;">
<li style="line-height: 160%; box-sizing: content-box; position: relative;">Dilated rate为质数或1</li>
<li style="line-height: 160%; box-sizing: content-box; position: relative;">将dilation rate设计成锯齿状结构，例如[1,2,5,1,2,5]循环</li>
<li style="line-height: 160%; box-sizing: content-box; position: relative;">Mi=max[Mi+1 - 2ri, xxxx]</li>
</ul>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;"><strong style="line-height: 160%; box-sizing: content-box; font-weight: 700;">其它改进方法：Atrous Spatial Pyramid Pooling (ASPP)</strong><br/>
基于港中文+商汤PSPNet中的Pooling module，ASPP在网络decoder上对于不同尺度上用不同大小的dilation rate去抓取多尺度信息，每个尺度作为一个独立的分支，最后合并，连接一个卷积层作为输出。这样的实际有效笔辩了在encoder上冗余信息的获取。</p>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;"><img src="Convolutional kernels_files/Image [10].png" type="image/png" data-filename="Image.png"/></p>
<blockquote style="line-height: 160%; box-sizing: content-box; margin: 15px 0; border-left: 4px solid #ddd; padding: 0 15px; color: #777;">
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333; margin-top: 0; margin-bottom: 0;">kernel中间的间隔为rate-1<br/>
通常做padding，空洞卷积后，输出尺寸大小与正常conv一致<br/>
假设kernel大小k=3，rate=16，则padding=rate=16</p>
</blockquote>
<h4 style="line-height: 160%; box-sizing: content-box; font-size: 20px; color: #333;">6 Fractionally-strided convolution</h4>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;">In <a href="https://arxiv.org/pdf/1511.06434.pdf" style="line-height: 160%; box-sizing: content-box; text-decoration: underline; color: #5286bc;">DCGAN</a>:</p>
<blockquote style="line-height: 160%; box-sizing: content-box; margin: 15px 0; border-left: 4px solid #ddd; padding: 0 15px; color: #777;">
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333; margin-top: 0; margin-bottom: 0;">Figure 1: DCGAN generator used for LSUN scene modeling. A 100 dimensional uniform distribution Z is <strong style="line-height: 160%; box-sizing: content-box; font-weight: 700;">projected to a small spatial extent convolutional representation</strong> with many feature maps. A series of <strong style="line-height: 160%; box-sizing: content-box; font-weight: 700;">four fractionally-strided convolutions (in some recent papers, these are wrongly called deconvolutions)</strong> then convert this high level representation into a 64*64 pixel image.</p>
</blockquote>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;"><a href="https://beerensahu.wordpress.com/2018/04/10/pytorch-a-fractionally-strided-convolution-or-a-deconvolution/" style="line-height: 160%; box-sizing: content-box; text-decoration: underline; color: #5286bc;">other</a></p>
<blockquote style="line-height: 160%; box-sizing: content-box; margin: 15px 0; border-left: 4px solid #ddd; padding: 0 15px; color: #777;">
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333; margin-top: 0; margin-bottom: 0;">I heard the term “fractionally- strided convolution” while studying GAN’s and Fully Convolutional Network (FCN). Some also refer this as a Deconvolution or transposed convolution. Transposed convolution is commonly used for up-sampling an input image.Prior to the use of transposed convolution for up-sampling, un-pooling was used. As we know that pooling is popularly used for down sampling input feature maps in CNN. Similarly un-pooling is the exact opposite process to up-sample. Like pooling, un-pooling also does not involve learning. However transposed convolution is learnable.</p>
</blockquote>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;"><a href="https://buptldy.github.io/2016/10/29/2016-10-29-deconv/" style="line-height: 160%; box-sizing: content-box; text-decoration: underline; color: #5286bc;">BP-learnable of transposed convolution</a></p>
<blockquote style="line-height: 160%; box-sizing: content-box; margin: 15px 0; border-left: 4px solid #ddd; padding: 0 15px; color: #777;">
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333; margin-top: 0; margin-bottom: 0;">我们已经知道卷积层的前向操作可以表示为和矩阵CC相乘，那么 我们很容易得到卷积层的反向传播就是和CC的转置相乘。</p>
</blockquote>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;"><strong style="line-height: 160%; box-sizing: content-box; font-weight: 700;">反卷积和卷积的关系</strong></p>
<blockquote style="line-height: 160%; box-sizing: content-box; margin: 15px 0; border-left: 4px solid #ddd; padding: 0 15px; color: #777;">
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333; margin-top: 0; margin-bottom: 0;">反卷积又被称为Transposed(转置) Convolution，我们可以看出其实卷积层的前向传播过程就是反卷积层的反向传播过程，卷积层的反向传播过程就是反卷积层的前向传播过程。因为卷积层的前向反向计算分别为乘 C和 CT,而反卷积层的前向反向计算分别为乘 CT 和 (CT)T，所以它们的前向传播和反向传播刚好交换过来。</p>
</blockquote>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;"><u style="line-height: 160%; box-sizing: content-box;"><em style="line-height: 160%; box-sizing: content-box; font-style: italic;"><strong style="line-height: 160%; box-sizing: content-box; font-weight: 700;">Fractionally-strided convolution 的stride大于1，正如下文transposed convolution示例图所示，fully-strided convolution的stride=1，input中间内部不插入0</strong></em></u></p>
<h4 style="line-height: 160%; box-sizing: content-box; font-size: 20px; color: #333;">7 project and reshape</h4>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;">According to GAN, project and reshape implementation including:</p>
<ul style="line-height: 160%; box-sizing: content-box; display: block; list-style-type: disc; padding-left: 30px; margin: 6px 0 10px; color: #333;">
<li style="line-height: 160%; box-sizing: content-box; position: relative;">Dense(100, 4*4*1024)</li>
<li style="line-height: 160%; box-sizing: content-box; position: relative;">reshape(4,4,1024)</li>
</ul>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;"><img src="Convolutional kernels_files/Image [11].png" type="image/png" data-filename="Image.png"/></p>
<h4 style="line-height: 160%; box-sizing: content-box; font-size: 20px; color: #333;">8 Shift Conv</h4>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;">在Shift卷积算子中，其基本思路也是类似于深度可分离卷积的设计，将卷积分为空间域和通道域的卷积，通道域的卷积同样是通过1x1卷积实现的，而在空间域卷积中，引入了shift操作。<br/>
<img src="Convolutional kernels_files/Image [12].png" type="image/png" data-filename="Image.png"/></p>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;">shift卷积的每一个卷积核都是一个“独热”的算子，其卷积核只有一个元素为1，其他全部为0。<br/>
对于输入的M个通道的张量，分别对应了M个Shift卷积核，如图中的不同颜色的卷积核所示。<br/>
我们把其中一个通道的shift卷积操作拿出来分析，如下图所示。我们发现，shift卷积过程相当于将原输入的矩阵在某个方向进行平移，这也是为什么该操作称之为shift的原因。虽然简单的平移操作似乎没有提取到空间信息，但是考虑到我们之前说到的，通道域是空间域信息的层次化扩散。因此通过设置不同方向的shift卷积核，可以将输入张量不同通道进行平移，随后配合1x1卷积实现跨通道的信息融合，即可实现空间域和通道域的信息提取。<br/>
<img src="Convolutional kernels_files/Image [13].png" type="image/png" data-filename="Image.png"/></p>
<h3 style="line-height: 160%; box-sizing: content-box; font-weight: 700; font-size: 27px; color: #333;">Deconvolutions</h3>
<h4 style="line-height: 160%; box-sizing: content-box; font-size: 20px; color: #333;">1. Upsample</h4>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;">实现图像由小分辨率到大分辨率的映射的操作，叫做上采样(Upsample)。上采样有3种常见的方法：线性插值（1个方向），双线性插值(bilinear，两个方向)，三次样条插值</p>
<h4 style="line-height: 160%; box-sizing: content-box; font-size: 20px; color: #333;">2. Transposed convolution</h4>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;">反卷积，也叫转置卷积，它并不是正向卷积的完全逆过程<br/>
用一句话来解释：反卷积是一种<strong style="line-height: 160%; box-sizing: content-box; font-weight: 700;">特殊的正向卷积</strong>，先按照一定的比例通过<strong style="line-height: 160%; box-sizing: content-box; font-weight: 700;">补0</strong>来扩大输入图像的尺寸，接着<strong style="line-height: 160%; box-sizing: content-box; font-weight: 700;">旋转卷积核</strong>，再进行<strong style="line-height: 160%; box-sizing: content-box; font-weight: 700;">正向卷积</strong>（padding包含stride参数，卷积核一般大于2*stride，下图stride等于2）。<br/>
<img src="Convolutional kernels_files/Image [14].png" type="image/png" data-filename="Image.png"/><br/>
相对upsample，能获得更大的感受野reception field</p>
<h4 style="line-height: 160%; box-sizing: content-box; font-size: 20px; color: #333;">3. keras.layers.Upsample2D</h4>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;">采用的是上采样方法</p>
<h4 style="line-height: 160%; box-sizing: content-box; font-size: 20px; color: #333;">4. tensorflow.nn.conv2d_transpose</h4>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;">反卷积</p>
</div><center style="display:none !important;visibility:collapse !important;height:0 !important;white-space:nowrap;width:100%;overflow:hidden">%5BTOC%5D%0A%0A%23%23%23%20Convolutions%0A%23%23%23%23%201%20Group%20convolution%0A%3E%E6%9C%80%E6%97%A9%E8%A7%81%E4%BA%8EAlexNet%EF%BC%8C%E7%94%A8%E6%9D%A5%E5%88%87%E5%88%86%E7%BD%91%E7%BB%9C%E5%9C%A8%E4%B8%A4%E4%B8%AAGPU%E4%B8%8A%E8%AE%AD%E7%BB%83%EF%BC%8C%E6%8E%A8%E5%B9%BF%3A%20%5BShuffleNet%5D(https%3A%2F%2Farxiv.org%2Fabs%2F1707.01083)%2C%20extremely%20efficient%20for%20mobile%20devices%0A%0AAlexNet%EF%BC%9A%0A!%5Be28d2e3e197ac212a434964a9332b617.png%5D(en-resource%3A%2F%2Fdatabase%2F881%3A1)%0AGroup%20conv%3A%0A!%5B574eeea7709878c931c181223576a393.png%5D(en-resource%3A%2F%2Fdatabase%2F883%3A1)%0A%0A**%E4%BC%98%E5%8A%BF%EF%BC%9A**%0A-%20%E5%87%8F%E5%B0%91%E5%8F%82%E6%95%B0%E9%87%8F%EF%BC%8C%E5%87%8F%E5%B0%91%E8%AE%A1%E7%AE%97%E9%87%8F%0A-%20Group%20convolution%E5%8F%AF%E4%BB%A5%E7%9C%8B%E6%88%90%E6%98%AFstructure%20sparse%EF%BC%8C%E7%9B%B8%E5%BD%93%E4%BA%8E%E6%AD%A3%E5%88%99%EF%BC%8C%E5%9C%A8%E5%87%8F%E5%B0%91%E5%8F%82%E6%95%B0%E9%87%8F%E7%9A%84%E5%90%8C%E6%97%B6%E8%8E%B7%E5%BE%97%E6%9B%B4%E5%A5%BD%E7%9A%84%E6%95%88%E6%9E%9C%0A-%20%E5%BD%93%E5%88%86%E7%BB%84%E6%95%B0%E7%AD%89%E4%BA%8E%E8%BE%93%E5%85%A5map%E6%95%B0%E9%87%8F%E6%97%B6%EF%BC%8C%E7%AD%89%E4%BB%B7%E4%BA%8Edepthwise%20convolution%EF%BC%8C%E5%8F%82%E6%95%B0%E9%87%8F%E8%BF%9B%E4%B8%80%E6%AD%A5%E5%87%8F%E5%B0%91%0A-%20%E5%85%B6%E5%AE%83%E6%83%85%E5%86%B5%E5%8F%AF%E7%AD%89%E4%BB%B7%E4%BA%8EGAP%E7%AD%89%0A!%5Bd6e51eb571c41c407aad9379e73b6324.png%5D(en-resource%3A%2F%2Fdatabase%2F885%3A1)%0A!%5Bd1da58a0215b35ce4738aab585450279.png%5D(en-resource%3A%2F%2Fdatabase%2F887%3A1)%0A%0A%23%23%23%23%202%20Depthwise%20(DW)%2FPointwise%20convolution%20(PW)%0A%3E%E6%8F%90%E5%87%BA%3A%20%5BMobileNet%5D(http%3A%2F%2Farxiv.org%2Fabs%2F1704.04861)%2C%20%E6%8F%90%E9%AB%98%E8%AE%AD%E7%BB%83%E9%80%9F%E5%BA%A6%E4%B8%8E%E6%95%88%E7%8E%87%EF%BC%8Cspeed%26accuracy%26efficiency%0A%0ADepthwise%20Separable%20Convolution%E6%98%AF%E5%B0%86%E4%B8%80%E4%B8%AA%E5%AE%8C%E6%95%B4%E7%9A%84%E5%8D%B7%E7%A7%AF%E8%BF%90%E7%AE%97%E5%88%86%E8%A7%A3%E4%B8%BA%E4%B8%A4%E6%AD%A5%E8%BF%9B%E8%A1%8C%EF%BC%8C%E5%8D%B3Depthwise%20Convolution%E4%B8%8EPointwise%20Convolution%E3%80%82%0A%0A-%20**Depthwise%20convolution%3A**%0ADepthwise%20Convolution%E7%9A%84%E4%B8%80%E4%B8%AA%E5%8D%B7%E7%A7%AF%E6%A0%B8%E8%B4%9F%E8%B4%A3%E4%B8%80%E4%B8%AA%E9%80%9A%E9%81%93%EF%BC%8C%E4%B8%80%E4%B8%AA%E9%80%9A%E9%81%93%E5%8F%AA%E8%A2%AB%E4%B8%80%E4%B8%AA%E5%8D%B7%E7%A7%AF%E6%A0%B8%E5%8D%B7%E7%A7%AF%E3%80%82%0A!%5B12f6584045e616045440735612a99616.png%5D(en-resource%3A%2F%2Fdatabase%2F889%3A1)%0A%0A-%20**Pointwise%20convolution**%0APointwise%20Convolution%E7%9A%84%E8%BF%90%E7%AE%97**%E4%B8%8E%E5%B8%B8%E8%A7%84%E5%8D%B7%E7%A7%AF%E8%BF%90%E7%AE%97%E9%9D%9E%E5%B8%B8%E7%9B%B8%E4%BC%BC(kernel%20size%E7%9A%84%E5%8C%BA%E5%88%AB)**%20%EF%BC%8C%E5%AE%83%E7%9A%84%E5%8D%B7%E7%A7%AF%E6%A0%B8%E7%9A%84%E5%B0%BA%E5%AF%B8%E4%B8%BA%201%C3%971%C3%97M%EF%BC%8CM%E4%B8%BA%E4%B8%8A%E4%B8%80%E5%B1%82%E7%9A%84%E9%80%9A%E9%81%93%E6%95%B0%E3%80%82%E6%89%80%E4%BB%A5%E8%BF%99%E9%87%8C%E7%9A%84%E5%8D%B7%E7%A7%AF%E8%BF%90%E7%AE%97%E4%BC%9A%E5%B0%86%E4%B8%8A%E4%B8%80%E6%AD%A5%E7%9A%84map%E5%9C%A8%E6%B7%B1%E5%BA%A6%E6%96%B9%E5%90%91%E4%B8%8A%E8%BF%9B%E8%A1%8C%E5%8A%A0%E6%9D%83%E7%BB%84%E5%90%88%EF%BC%8C%E7%94%9F%E6%88%90%E6%96%B0%E7%9A%84Feature%20map%E3%80%82%E6%9C%89%E5%87%A0%E4%B8%AA%E5%8D%B7%E7%A7%AF%E6%A0%B8%E5%B0%B1%E6%9C%89%E5%87%A0%E4%B8%AA%E8%BE%93%E5%87%BAFeature%20map%E3%80%82%0A!%5B89f0c68058f5f95b64655d017d4fe03c.png%5D(en-resource%3A%2F%2Fdatabase%2F891%3A1)%0A%0A%3E%E5%9B%9E%E9%A1%BE%E4%B8%80%E4%B8%8B%EF%BC%8C%E5%B8%B8%E8%A7%84%E5%8D%B7%E7%A7%AF%E7%9A%84**%E5%8F%82%E6%95%B0%E4%B8%AA%E6%95%B0**%E4%B8%BA%EF%BC%9A%0AN_std%20%3D%204%20%C3%97%203%20%C3%97%203%20%C3%97%203%20%3D%20108%0ASeparable%20Convolution%E7%9A%84%E5%8F%82%E6%95%B0%E7%94%B1%E4%B8%A4%E9%83%A8%E5%88%86%E7%9B%B8%E5%8A%A0%E5%BE%97%E5%88%B0%EF%BC%9A%0AN_depthwise%20%3D%203%20%C3%97%203%20%C3%97%203%20%3D%2027%0AN_pointwise%20%3D%201%20%C3%97%201%20%C3%97%203%20%C3%97%204%20%3D%2012%0AN_separable%20%3D%20N_depthwise%20%2B%20N_pointwise%20%3D%2039%0A%E7%9B%B8%E5%90%8C%E7%9A%84%E8%BE%93%E5%85%A5%EF%BC%8C%E5%90%8C%E6%A0%B7%E6%98%AF%E5%BE%97%E5%88%B04%E5%BC%A0Feature%20map%EF%BC%8CSeparable%20Convolution%E7%9A%84%E5%8F%82%E6%95%B0%E4%B8%AA%E6%95%B0%E6%98%AF%E5%B8%B8%E8%A7%84%E5%8D%B7%E7%A7%AF%E7%9A%84%E7%BA%A61%2F3%E3%80%82%E5%9B%A0%E6%AD%A4%EF%BC%8C**%E5%9C%A8%E5%8F%82%E6%95%B0%E9%87%8F%E7%9B%B8%E5%90%8C%E7%9A%84%E5%89%8D%E6%8F%90%E4%B8%8B%EF%BC%8C%E9%87%87%E7%94%A8Separable%20Convolution%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%B1%82%E6%95%B0%E5%8F%AF%E4%BB%A5%E5%81%9A%E7%9A%84%E6%9B%B4%E6%B7%B1%E3%80%82**%0A%0A%23%23%23%23%203%20Inverted%20residual%20block%20%26%26%20Linear%20Bottlenecks%0A%3E%E6%8F%90%E5%87%BA%3A%20%5BMobleNet-V2%5D(https%3A%2F%2Farxiv.org%2Fabs%2F1801.04381)%EF%BC%8Cimprovement%20in%20terms%20of%20accuracy%20and%20speed%0A%0A!%5B2f09095bb2c6035890560b8ee9b7f808.png%5D(en-resource%3A%2F%2Fdatabase%2F901%3A1)%0A%0A**%E4%BC%A0%E7%BB%9F%E7%9A%84residual%20block**%EF%BC%8C%E5%85%88%E7%94%A81x1%E5%8D%B7%E7%A7%AF%E5%B0%86%E8%BE%93%E5%85%A5%E7%9A%84feature%20map%E7%9A%84**%E7%BB%B4%E5%BA%A6%E9%99%8D%E4%BD%8E**%EF%BC%8C%E7%84%B6%E5%90%8E%E8%BF%9B%E8%A1%8C3x3%E7%9A%84%E5%8D%B7%E7%A7%AF%E6%93%8D%E4%BD%9C%EF%BC%8C%E6%9C%80%E5%90%8E%E5%86%8D%E7%94%A81x1%E7%9A%84%E5%8D%B7%E7%A7%AF%E5%B0%86**%E7%BB%B4%E5%BA%A6%E5%8F%98%E5%A4%A7**%E3%80%82%0A**Inverted%20residual%20block**%E5%85%88%E7%94%A81x1%E5%8D%B7%E7%A7%AF%E5%B0%86%E8%BE%93%E5%85%A5%E7%9A%84feature%20map**%E7%BB%B4%E5%BA%A6%E5%8F%98%E5%A4%A7**%EF%BC%8C%E7%84%B6%E5%90%8E%E7%94%A83x3%20**depthwise%20convolution**%E6%96%B9%E5%BC%8F%E5%81%9A%E5%8D%B7%E7%A7%AF%E8%BF%90%E7%AE%97%EF%BC%8C%E6%9C%80%E5%90%8E%E4%BD%BF%E7%94%A81x1%E7%9A%84%E5%8D%B7%E7%A7%AF%E8%BF%90%E7%AE%97%E5%B0%86%E5%85%B6**%E7%BB%B4%E5%BA%A6%E7%BC%A9%E5%B0%8F**%E3%80%82%E6%B3%A8%E6%84%8F%EF%BC%8C%E6%AD%A4%E6%97%B6%E7%9A%841x1%E5%8D%B7%E7%A7%AF%E8%BF%90%E7%AE%97%E5%90%8E%EF%BC%8C**%E4%B8%8D%E5%86%8D%E4%BD%BF%E7%94%A8ReLU%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0**%EF%BC%8C%E8%80%8C%E6%98%AF%E4%BD%BF%E7%94%A8**%E7%BA%BF%E6%80%A7%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%20(linear%20bottleneck)**%EF%BC%8C%E4%BB%A5%E4%BF%9D%E7%95%99%E6%9B%B4%E5%A4%9A%E7%89%B9%E5%BE%81%E4%BF%A1%E6%81%AF%EF%BC%8C%E4%BF%9D%E8%AF%81%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%A1%A8%E8%BE%BE%E8%83%BD%E5%8A%9B%E3%80%82%0A%0A%23%23%23%23%204%20Fire%20module%0A%3E%E6%8F%90%E5%87%BA%3A%20%5BSqueezenet%5D(https%3A%2F%2Farxiv.org%2Fabs%2F1602.07360)%2C%20%E5%AE%9E%E7%8E%B0%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%EF%BC%8C%E5%87%8F%E5%B0%91%E5%8F%82%E6%95%B0%E9%87%8F50x%0A%0A!%5Bfc58cf6048910f27b9b5f1c255d14fd2.png%5D(en-resource%3A%2F%2Fdatabase%2F893%3A1)%0A%0A%E5%9C%A8Fire%20module%E4%B8%AD%E4%BD%9C%E8%80%85%E4%BD%BF%E7%94%A8%E4%BA%86**%E4%B8%89%E4%B8%AAhyper%20parameters**%E7%94%A8%E4%BA%8E%E8%A1%A8%E7%A4%BA%E5%AE%83%E7%9A%84%E6%9E%84%E6%88%90%E3%80%82%0As1x1%E8%A1%A8%E7%A4%BA**squeeze%20layer%20filters**%E7%9A%84%E6%95%B0%E7%9B%AE%EF%BC%9Be1x1%E8%A1%A8%E7%A4%BA**expand%20layer**%E4%B8%AD1x1%20conv%20filters%E7%9A%84%E6%95%B0%E7%9B%AE%EF%BC%8Ce3x3%E5%88%99%E8%A1%A8%E7%A4%BAexpand%20layer%E4%B8%AD3x3%20conv%20filters%E7%9A%84%E6%95%B0%E7%9B%AE%E3%80%82%0A%E5%9B%A0%E4%B8%BA%E5%9C%A8%E6%AF%8F%E4%B8%AAfire%20module%E5%86%85%E9%83%A8s1x1%E8%A6%81%E8%BF%9C%E5%B0%8F%E4%BA%8Ee1x1%20%2B%20e3x3%EF%BC%8C%E5%AE%83%E4%BB%AC%E6%BB%A1%E8%B6%B3s1x1%20%3D%20SR%20*%20(e1x1%20%2B%20e3x3)%E3%80%82%E8%80%8C**SR%E7%A7%B0%E4%B8%BA%E7%BC%A9%E5%87%8F%E7%B3%BB%E6%95%B0**%EF%BC%8C%E5%9C%A8%E8%BF%99%E9%87%8C%E5%8F%AA%E6%9C%890.125%E3%80%82%0A%0A%0A%23%23%23%23%205%20Dilated%2FAtrous%20convolution%0A%3E%E6%8F%90%E5%87%BA%3A%20%5BMulti-Scale%20Context%20Aggregation%20by%20Dilated%20Convolutions%5D(https%3A%2F%2Farxiv.org%2Fabs%2F1511.07122)%2C%20%E5%B8%B8%E7%94%A8%E4%BA%8E%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%0AVGG%E4%B8%AD%E7%9A%84%E5%8F%91%E7%8E%B0%EF%BC%9A%C2%A07%20x%207%20%E7%9A%84%E5%8D%B7%E7%A7%AF%E5%B1%82%E7%9A%84%E6%AD%A3%E5%88%99%E7%AD%89%E6%95%88%E4%BA%8E%203%20%E4%B8%AA%203%20x%203%20%E7%9A%84%E5%8D%B7%E7%A7%AF%E5%B1%82%E7%9A%84%E5%8F%A0%E5%8A%A0%EF%BC%8C%E8%BF%99%E6%A0%B7%E7%9A%84%E8%AE%BE%E8%AE%A1%E4%B8%8D%E4%BB%85%E5%8F%AF%E4%BB%A5%E5%A4%A7%E5%B9%85%E5%BA%A6%E7%9A%84**%E5%87%8F%E5%B0%91%E5%8F%82%E6%95%B0**%EF%BC%8C%E5%85%B6%E6%9C%AC%E8%BA%AB%E5%B8%A6%E6%9C%89**%E6%AD%A3%E5%88%99%E6%80%A7%E8%B4%A8**%E7%9A%84%20convolution%20map%20%E8%83%BD%E5%A4%9F%E6%9B%B4%E5%AE%B9%E6%98%93%E5%AD%A6%E4%B8%80%E4%B8%AA%20generlisable%2C%20expressive%20feature%20space%E3%80%82%E8%BF%99%E4%B9%9F%E6%98%AF%E7%8E%B0%E5%9C%A8%E7%BB%9D%E5%A4%A7%E9%83%A8%E5%88%86%E5%9F%BA%E4%BA%8E%E5%8D%B7%E7%A7%AF%E7%9A%84**%E6%B7%B1%E5%B1%82%E7%BD%91%E7%BB%9C**%E9%83%BD%E5%9C%A8%E7%94%A8**%E5%B0%8F%E5%8D%B7%E7%A7%AF%E6%A0%B8**%E7%9A%84%E5%8E%9F%E5%9B%A0%E3%80%82%0A!%5B57a855d663390d5e9e0c2b351cf6741a.png%5D(en-resource%3A%2F%2Fdatabase%2F895%3A1)%0A%0A%E7%84%B6%E8%80%8C%20Deep%20CNN%20%E5%AF%B9%E4%BA%8E%E5%85%B6%E4%BB%96%E4%BB%BB%E5%8A%A1%E8%BF%98%E6%9C%89%E4%B8%80%E4%BA%9B%E8%87%B4%E5%91%BD%E6%80%A7%E7%9A%84%E7%BC%BA%E9%99%B7%E3%80%82%E8%BE%83%E4%B8%BA%E8%91%97%E5%90%8D%E7%9A%84%E6%98%AF%20up-sampling%20%E5%92%8C%20pooling%20layer%20%E7%9A%84%E8%AE%BE%E8%AE%A1%E3%80%82%E8%BF%99%E4%B8%AA%E5%9C%A8%20Hinton%20%E7%9A%84%E6%BC%94%E8%AE%B2%E9%87%8C%E4%B9%9F%E4%B8%80%E7%9B%B4%E6%8F%90%E5%88%B0%E8%BF%87%E3%80%82%E4%B8%BB%E8%A6%81%E9%97%AE%E9%A2%98%E6%9C%89%EF%BC%9A%0A-%20Up-sampling%20%2F%20pooling%20layer%20(e.g.%20bilinear%20interpolation)%20is%20deterministic.%20(a.k.a.%20not%20learnable)%0A-%20%E5%86%85%E9%83%A8%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%A2%E5%A4%B1%EF%BC%9B%E7%A9%BA%E9%97%B4%E5%B1%82%E7%BA%A7%E5%8C%96%E4%BF%A1%E6%81%AF%E4%B8%A2%E5%A4%B1%E3%80%82%0A-%20**%E5%B0%8F%E7%89%A9%E4%BD%93%E4%BF%A1%E6%81%AF%E6%97%A0%E6%B3%95%E9%87%8D%E5%BB%BA**%20(%E5%81%87%E8%AE%BE%E6%9C%89%E5%9B%9B%E4%B8%AApooling%20layer%20%E5%88%99%20%E4%BB%BB%E4%BD%95%E5%B0%8F%E4%BA%8E%202%5E4%20%3D%2016%20pixel%20%E7%9A%84%E7%89%A9%E4%BD%93%E4%BF%A1%E6%81%AF%E5%B0%86%E7%90%86%E8%AE%BA%E4%B8%8A%E6%97%A0%E6%B3%95%E9%87%8D%E5%BB%BA%E3%80%82)%0A%0A%E5%9C%A8%E8%BF%99%E6%A0%B7%E9%97%AE%E9%A2%98%E7%9A%84%E5%AD%98%E5%9C%A8%E4%B8%8B%EF%BC%8C**%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E9%97%AE%E9%A2%98**%E4%B8%80%E7%9B%B4%E5%A4%84%E5%9C%A8%E7%93%B6%E9%A2%88%E6%9C%9F%E6%97%A0%E6%B3%95%E5%86%8D%E6%98%8E%E6%98%BE%E6%8F%90%E9%AB%98%E7%B2%BE%E5%BA%A6%EF%BC%8C%20%E8%80%8C%20dilated%20convolution%20%E7%9A%84%E8%AE%BE%E8%AE%A1%E5%B0%B1%E8%89%AF%E5%A5%BD%E7%9A%84%E9%81%BF%E5%85%8D%E4%BA%86%E8%BF%99%E4%BA%9B%E9%97%AE%E9%A2%98%E3%80%82%0A%0A**Dilated%20convolution%E4%BC%98%E7%82%B9%EF%BC%9A**%0A%E5%86%85%E9%83%A8%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E7%9A%84%E4%BF%9D%E7%95%99%E5%92%8C%E9%81%BF%E5%85%8D%E4%BD%BF%E7%94%A8%20down-sampling%20%E8%BF%99%E6%A0%B7%E7%9A%84%E7%89%B9%E6%80%A7%E3%80%82%0A%0A!%5Bd7c195648cf2bd551607b8361eee4ba2.png%5D(en-resource%3A%2F%2Fdatabase%2F897%3A1)%0A%0A**%E7%BC%BA%E9%99%B7%EF%BC%9A**%0A-%20The%20gridding%20effect%2C%20related%20to%20dilation%20rate%0A-%20Long-ranged%20information%20might%20be%20not%20relevent%0A%0A%E5%9B%BE%E6%A3%AE%E7%BB%84%E7%9A%84%E6%96%87%E7%AB%A0%E7%9A%84solution%EF%BC%9A**Hybrid%20Dilated%20convolution%20(HDC)**%0A-%20Dilated%20rate%E4%B8%BA%E8%B4%A8%E6%95%B0%E6%88%961%0A-%20%E5%B0%86dilation%20rate%E8%AE%BE%E8%AE%A1%E6%88%90%E9%94%AF%E9%BD%BF%E7%8A%B6%E7%BB%93%E6%9E%84%EF%BC%8C%E4%BE%8B%E5%A6%82%5B1%2C2%2C5%2C1%2C2%2C5%5D%E5%BE%AA%E7%8E%AF%0A-%20Mi%3Dmax%5BMi%2B1%20-%202ri%2C%20xxxx%5D%0A%0A**%E5%85%B6%E5%AE%83%E6%94%B9%E8%BF%9B%E6%96%B9%E6%B3%95%EF%BC%9AAtrous%20Spatial%20Pyramid%20Pooling%20(ASPP)**%0A%E5%9F%BA%E4%BA%8E%E6%B8%AF%E4%B8%AD%E6%96%87%2B%E5%95%86%E6%B1%A4PSPNet%E4%B8%AD%E7%9A%84Pooling%20module%EF%BC%8CASPP%E5%9C%A8%E7%BD%91%E7%BB%9Cdecoder%E4%B8%8A%E5%AF%B9%E4%BA%8E%E4%B8%8D%E5%90%8C%E5%B0%BA%E5%BA%A6%E4%B8%8A%E7%94%A8%E4%B8%8D%E5%90%8C%E5%A4%A7%E5%B0%8F%E7%9A%84dilation%20rate%E5%8E%BB%E6%8A%93%E5%8F%96%E5%A4%9A%E5%B0%BA%E5%BA%A6%E4%BF%A1%E6%81%AF%EF%BC%8C%E6%AF%8F%E4%B8%AA%E5%B0%BA%E5%BA%A6%E4%BD%9C%E4%B8%BA%E4%B8%80%E4%B8%AA%E7%8B%AC%E7%AB%8B%E7%9A%84%E5%88%86%E6%94%AF%EF%BC%8C%E6%9C%80%E5%90%8E%E5%90%88%E5%B9%B6%EF%BC%8C%E8%BF%9E%E6%8E%A5%E4%B8%80%E4%B8%AA%E5%8D%B7%E7%A7%AF%E5%B1%82%E4%BD%9C%E4%B8%BA%E8%BE%93%E5%87%BA%E3%80%82%E8%BF%99%E6%A0%B7%E7%9A%84%E5%AE%9E%E9%99%85%E6%9C%89%E6%95%88%E7%AC%94%E8%BE%A9%E4%BA%86%E5%9C%A8encoder%E4%B8%8A%E5%86%97%E4%BD%99%E4%BF%A1%E6%81%AF%E7%9A%84%E8%8E%B7%E5%8F%96%E3%80%82%0A%0A!%5B8fd90fd669e3c1c1609ca09363873dc3.png%5D(en-resource%3A%2F%2Fdatabase%2F899%3A1)%0A%0A%3Ekernel%E4%B8%AD%E9%97%B4%E7%9A%84%E9%97%B4%E9%9A%94%E4%B8%BArate-1%0A%3E%E9%80%9A%E5%B8%B8%E5%81%9Apadding%EF%BC%8C%E7%A9%BA%E6%B4%9E%E5%8D%B7%E7%A7%AF%E5%90%8E%EF%BC%8C%E8%BE%93%E5%87%BA%E5%B0%BA%E5%AF%B8%E5%A4%A7%E5%B0%8F%E4%B8%8E%E6%AD%A3%E5%B8%B8conv%E4%B8%80%E8%87%B4%0A%3E%E5%81%87%E8%AE%BEkernel%E5%A4%A7%E5%B0%8Fk%3D3%EF%BC%8Crate%3D16%EF%BC%8C%E5%88%99padding%3Drate%3D16%0A%0A%23%23%23%23%206%20Fractionally-strided%20convolution%0AIn%20%5BDCGAN%5D(https%3A%2F%2Farxiv.org%2Fpdf%2F1511.06434.pdf)%3A%0A%3EFigure%201%3A%20DCGAN%20generator%20used%20for%20LSUN%20scene%20modeling.%20A%20100%20dimensional%20uniform%20distribution%20Z%20is%20**projected%20to%20a%20small%20spatial%20extent%20convolutional%20representation**%20with%20many%20feature%20maps.%20A%20series%20of%20**four%20fractionally-strided%20convolutions%20(in%20some%20recent%20papers%2C%20these%20are%20wrongly%20called%20deconvolutions)**%20then%20convert%20this%20high%20level%20representation%20into%20a%2064%5C*64%20pixel%20image.%0A%5Bother%5D(https%3A%2F%2Fbeerensahu.wordpress.com%2F2018%2F04%2F10%2Fpytorch-a-fractionally-strided-convolution-or-a-deconvolution%2F)%0A%3EI%20heard%20the%20term%20%E2%80%9Cfractionally-%20strided%20convolution%E2%80%9D%20while%20studying%20GAN%E2%80%99s%20and%20Fully%20Convolutional%20Network%20(FCN).%20Some%20also%20refer%20this%20as%20a%20Deconvolution%20or%20transposed%20convolution.%20Transposed%20convolution%20is%20commonly%20used%20for%20up-sampling%20an%20input%20image.Prior%20to%20the%20use%20of%20transposed%20convolution%20for%20up-sampling%2C%20un-pooling%20was%20used.%20As%20we%20know%20that%20pooling%20is%20popularly%20used%20for%20down%20sampling%20input%20feature%20maps%20in%20CNN.%20Similarly%20un-pooling%20is%20the%20exact%20opposite%20process%20to%20up-sample.%20Like%20pooling%2C%20un-pooling%20also%20does%20not%20involve%20learning.%20However%20transposed%20convolution%20is%20learnable.%0A%5BBP-learnable%20of%20transposed%20convolution%5D(https%3A%2F%2Fbuptldy.github.io%2F2016%2F10%2F29%2F2016-10-29-deconv%2F)%0A%3E%E6%88%91%E4%BB%AC%E5%B7%B2%E7%BB%8F%E7%9F%A5%E9%81%93%E5%8D%B7%E7%A7%AF%E5%B1%82%E7%9A%84%E5%89%8D%E5%90%91%E6%93%8D%E4%BD%9C%E5%8F%AF%E4%BB%A5%E8%A1%A8%E7%A4%BA%E4%B8%BA%E5%92%8C%E7%9F%A9%E9%98%B5CC%E7%9B%B8%E4%B9%98%EF%BC%8C%E9%82%A3%E4%B9%88%C2%A0%E6%88%91%E4%BB%AC%E5%BE%88%E5%AE%B9%E6%98%93%E5%BE%97%E5%88%B0%E5%8D%B7%E7%A7%AF%E5%B1%82%E7%9A%84%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E5%B0%B1%E6%98%AF%E5%92%8CCC%E7%9A%84%E8%BD%AC%E7%BD%AE%E7%9B%B8%E4%B9%98%E3%80%82%0A%0A**%E5%8F%8D%E5%8D%B7%E7%A7%AF%E5%92%8C%E5%8D%B7%E7%A7%AF%E7%9A%84%E5%85%B3%E7%B3%BB**%0A%3E%E5%8F%8D%E5%8D%B7%E7%A7%AF%E5%8F%88%E8%A2%AB%E7%A7%B0%E4%B8%BATransposed(%E8%BD%AC%E7%BD%AE)%20Convolution%EF%BC%8C%E6%88%91%E4%BB%AC%E5%8F%AF%E4%BB%A5%E7%9C%8B%E5%87%BA%E5%85%B6%E5%AE%9E%E5%8D%B7%E7%A7%AF%E5%B1%82%E7%9A%84%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%BF%87%E7%A8%8B%E5%B0%B1%E6%98%AF%E5%8F%8D%E5%8D%B7%E7%A7%AF%E5%B1%82%E7%9A%84%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%BF%87%E7%A8%8B%EF%BC%8C%E5%8D%B7%E7%A7%AF%E5%B1%82%E7%9A%84%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%BF%87%E7%A8%8B%E5%B0%B1%E6%98%AF%E5%8F%8D%E5%8D%B7%E7%A7%AF%E5%B1%82%E7%9A%84%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%BF%87%E7%A8%8B%E3%80%82%E5%9B%A0%E4%B8%BA%E5%8D%B7%E7%A7%AF%E5%B1%82%E7%9A%84%E5%89%8D%E5%90%91%E5%8F%8D%E5%90%91%E8%AE%A1%E7%AE%97%E5%88%86%E5%88%AB%E4%B8%BA%E4%B9%98%C2%A0C%E5%92%8C%C2%A0CT%2C%E8%80%8C%E5%8F%8D%E5%8D%B7%E7%A7%AF%E5%B1%82%E7%9A%84%E5%89%8D%E5%90%91%E5%8F%8D%E5%90%91%E8%AE%A1%E7%AE%97%E5%88%86%E5%88%AB%E4%B8%BA%E4%B9%98%C2%A0CT%C2%A0%E5%92%8C%C2%A0(CT)T%EF%BC%8C%E6%89%80%E4%BB%A5%E5%AE%83%E4%BB%AC%E7%9A%84%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E5%88%9A%E5%A5%BD%E4%BA%A4%E6%8D%A2%E8%BF%87%E6%9D%A5%E3%80%82%0A%0A%3Cu%3E***Fractionally-strided%20convolution%20%E7%9A%84stride%E5%A4%A7%E4%BA%8E1%EF%BC%8C%E6%AD%A3%E5%A6%82%E4%B8%8B%E6%96%87transposed%20convolution%E7%A4%BA%E4%BE%8B%E5%9B%BE%E6%89%80%E7%A4%BA%EF%BC%8Cfully-strided%20convolution%E7%9A%84stride%3D1%EF%BC%8Cinput%E4%B8%AD%E9%97%B4%E5%86%85%E9%83%A8%E4%B8%8D%E6%8F%92%E5%85%A50***%3C%2Fu%3E%0A%0A%0A%23%23%23%23%207%20project%20and%20reshape%0AAccording%20to%20GAN%2C%20project%20and%20reshape%20implementation%20including%3A%0A-%20Dense(100%2C%204%5C*4%5C*1024)%0A-%20reshape(4%2C4%2C1024)%0A%0A!%5Bc7b42908faf0d9a41db45c26552423dd.png%5D(en-resource%3A%2F%2Fdatabase%2F1015%3A1)%0A%0A%23%23%23%23%208%20Shift%20Conv%0A%E5%9C%A8Shift%E5%8D%B7%E7%A7%AF%E7%AE%97%E5%AD%90%E4%B8%AD%EF%BC%8C%E5%85%B6%E5%9F%BA%E6%9C%AC%E6%80%9D%E8%B7%AF%E4%B9%9F%E6%98%AF%E7%B1%BB%E4%BC%BC%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%8F%AF%E5%88%86%E7%A6%BB%E5%8D%B7%E7%A7%AF%E7%9A%84%E8%AE%BE%E8%AE%A1%EF%BC%8C%E5%B0%86%E5%8D%B7%E7%A7%AF%E5%88%86%E4%B8%BA%E7%A9%BA%E9%97%B4%E5%9F%9F%E5%92%8C%E9%80%9A%E9%81%93%E5%9F%9F%E7%9A%84%E5%8D%B7%E7%A7%AF%EF%BC%8C%E9%80%9A%E9%81%93%E5%9F%9F%E7%9A%84%E5%8D%B7%E7%A7%AF%E5%90%8C%E6%A0%B7%E6%98%AF%E9%80%9A%E8%BF%871x1%E5%8D%B7%E7%A7%AF%E5%AE%9E%E7%8E%B0%E7%9A%84%EF%BC%8C%E8%80%8C%E5%9C%A8%E7%A9%BA%E9%97%B4%E5%9F%9F%E5%8D%B7%E7%A7%AF%E4%B8%AD%EF%BC%8C%E5%BC%95%E5%85%A5%E4%BA%86shift%E6%93%8D%E4%BD%9C%E3%80%82%0A!%5B78b26e18bf04133ad8e09aabc7b0de8e.png%5D(en-resource%3A%2F%2Fdatabase%2F1373%3A0)%0A%0Ashift%E5%8D%B7%E7%A7%AF%E7%9A%84%E6%AF%8F%E4%B8%80%E4%B8%AA%E5%8D%B7%E7%A7%AF%E6%A0%B8%E9%83%BD%E6%98%AF%E4%B8%80%E4%B8%AA%E2%80%9C%E7%8B%AC%E7%83%AD%E2%80%9D%E7%9A%84%E7%AE%97%E5%AD%90%EF%BC%8C%E5%85%B6%E5%8D%B7%E7%A7%AF%E6%A0%B8%E5%8F%AA%E6%9C%89%E4%B8%80%E4%B8%AA%E5%85%83%E7%B4%A0%E4%B8%BA1%EF%BC%8C%E5%85%B6%E4%BB%96%E5%85%A8%E9%83%A8%E4%B8%BA0%E3%80%82%0A%E5%AF%B9%E4%BA%8E%E8%BE%93%E5%85%A5%E7%9A%84M%E4%B8%AA%E9%80%9A%E9%81%93%E7%9A%84%E5%BC%A0%E9%87%8F%EF%BC%8C%E5%88%86%E5%88%AB%E5%AF%B9%E5%BA%94%E4%BA%86M%E4%B8%AAShift%E5%8D%B7%E7%A7%AF%E6%A0%B8%EF%BC%8C%E5%A6%82%E5%9B%BE%E4%B8%AD%E7%9A%84%E4%B8%8D%E5%90%8C%E9%A2%9C%E8%89%B2%E7%9A%84%E5%8D%B7%E7%A7%AF%E6%A0%B8%E6%89%80%E7%A4%BA%E3%80%82%0A%E6%88%91%E4%BB%AC%E6%8A%8A%E5%85%B6%E4%B8%AD%E4%B8%80%E4%B8%AA%E9%80%9A%E9%81%93%E7%9A%84shift%E5%8D%B7%E7%A7%AF%E6%93%8D%E4%BD%9C%E6%8B%BF%E5%87%BA%E6%9D%A5%E5%88%86%E6%9E%90%EF%BC%8C%E5%A6%82%E4%B8%8B%E5%9B%BE%E6%89%80%E7%A4%BA%E3%80%82%E6%88%91%E4%BB%AC%E5%8F%91%E7%8E%B0%EF%BC%8Cshift%E5%8D%B7%E7%A7%AF%E8%BF%87%E7%A8%8B%E7%9B%B8%E5%BD%93%E4%BA%8E%E5%B0%86%E5%8E%9F%E8%BE%93%E5%85%A5%E7%9A%84%E7%9F%A9%E9%98%B5%E5%9C%A8%E6%9F%90%E4%B8%AA%E6%96%B9%E5%90%91%E8%BF%9B%E8%A1%8C%E5%B9%B3%E7%A7%BB%EF%BC%8C%E8%BF%99%E4%B9%9F%E6%98%AF%E4%B8%BA%E4%BB%80%E4%B9%88%E8%AF%A5%E6%93%8D%E4%BD%9C%E7%A7%B0%E4%B9%8B%E4%B8%BAshift%E7%9A%84%E5%8E%9F%E5%9B%A0%E3%80%82%E8%99%BD%E7%84%B6%E7%AE%80%E5%8D%95%E7%9A%84%E5%B9%B3%E7%A7%BB%E6%93%8D%E4%BD%9C%E4%BC%BC%E4%B9%8E%E6%B2%A1%E6%9C%89%E6%8F%90%E5%8F%96%E5%88%B0%E7%A9%BA%E9%97%B4%E4%BF%A1%E6%81%AF%EF%BC%8C%E4%BD%86%E6%98%AF%E8%80%83%E8%99%91%E5%88%B0%E6%88%91%E4%BB%AC%E4%B9%8B%E5%89%8D%E8%AF%B4%E5%88%B0%E7%9A%84%EF%BC%8C%E9%80%9A%E9%81%93%E5%9F%9F%E6%98%AF%E7%A9%BA%E9%97%B4%E5%9F%9F%E4%BF%A1%E6%81%AF%E7%9A%84%E5%B1%82%E6%AC%A1%E5%8C%96%E6%89%A9%E6%95%A3%E3%80%82%E5%9B%A0%E6%AD%A4%E9%80%9A%E8%BF%87%E8%AE%BE%E7%BD%AE%E4%B8%8D%E5%90%8C%E6%96%B9%E5%90%91%E7%9A%84shift%E5%8D%B7%E7%A7%AF%E6%A0%B8%EF%BC%8C%E5%8F%AF%E4%BB%A5%E5%B0%86%E8%BE%93%E5%85%A5%E5%BC%A0%E9%87%8F%E4%B8%8D%E5%90%8C%E9%80%9A%E9%81%93%E8%BF%9B%E8%A1%8C%E5%B9%B3%E7%A7%BB%EF%BC%8C%E9%9A%8F%E5%90%8E%E9%85%8D%E5%90%881x1%E5%8D%B7%E7%A7%AF%E5%AE%9E%E7%8E%B0%E8%B7%A8%E9%80%9A%E9%81%93%E7%9A%84%E4%BF%A1%E6%81%AF%E8%9E%8D%E5%90%88%EF%BC%8C%E5%8D%B3%E5%8F%AF%E5%AE%9E%E7%8E%B0%E7%A9%BA%E9%97%B4%E5%9F%9F%E5%92%8C%E9%80%9A%E9%81%93%E5%9F%9F%E7%9A%84%E4%BF%A1%E6%81%AF%E6%8F%90%E5%8F%96%E3%80%82%0A!%5B5ce51ccc19b2be56143b3d248a066514.png%5D(en-resource%3A%2F%2Fdatabase%2F1375%3A0)%0A%0A%0A%0A%23%23%23%20Deconvolutions%0A%23%23%23%23%201.%20Upsample%0A%E5%AE%9E%E7%8E%B0%E5%9B%BE%E5%83%8F%E7%94%B1%E5%B0%8F%E5%88%86%E8%BE%A8%E7%8E%87%E5%88%B0%E5%A4%A7%E5%88%86%E8%BE%A8%E7%8E%87%E7%9A%84%E6%98%A0%E5%B0%84%E7%9A%84%E6%93%8D%E4%BD%9C%EF%BC%8C%E5%8F%AB%E5%81%9A%E4%B8%8A%E9%87%87%E6%A0%B7(Upsample)%E3%80%82%E4%B8%8A%E9%87%87%E6%A0%B7%E6%9C%893%E7%A7%8D%E5%B8%B8%E8%A7%81%E7%9A%84%E6%96%B9%E6%B3%95%EF%BC%9A%E7%BA%BF%E6%80%A7%E6%8F%92%E5%80%BC%EF%BC%881%E4%B8%AA%E6%96%B9%E5%90%91%EF%BC%89%EF%BC%8C%E5%8F%8C%E7%BA%BF%E6%80%A7%E6%8F%92%E5%80%BC(bilinear%EF%BC%8C%E4%B8%A4%E4%B8%AA%E6%96%B9%E5%90%91)%EF%BC%8C%E4%B8%89%E6%AC%A1%E6%A0%B7%E6%9D%A1%E6%8F%92%E5%80%BC%0A%0A%23%23%23%23%202.%20Transposed%20convolution%0A%E5%8F%8D%E5%8D%B7%E7%A7%AF%EF%BC%8C%E4%B9%9F%E5%8F%AB%E8%BD%AC%E7%BD%AE%E5%8D%B7%E7%A7%AF%EF%BC%8C%E5%AE%83%E5%B9%B6%E4%B8%8D%E6%98%AF%E6%AD%A3%E5%90%91%E5%8D%B7%E7%A7%AF%E7%9A%84%E5%AE%8C%E5%85%A8%E9%80%86%E8%BF%87%E7%A8%8B%0A%E7%94%A8%E4%B8%80%E5%8F%A5%E8%AF%9D%E6%9D%A5%E8%A7%A3%E9%87%8A%EF%BC%9A%E5%8F%8D%E5%8D%B7%E7%A7%AF%E6%98%AF%E4%B8%80%E7%A7%8D**%E7%89%B9%E6%AE%8A%E7%9A%84%E6%AD%A3%E5%90%91%E5%8D%B7%E7%A7%AF**%EF%BC%8C%E5%85%88%E6%8C%89%E7%85%A7%E4%B8%80%E5%AE%9A%E7%9A%84%E6%AF%94%E4%BE%8B%E9%80%9A%E8%BF%87**%E8%A1%A50**%E6%9D%A5%E6%89%A9%E5%A4%A7%E8%BE%93%E5%85%A5%E5%9B%BE%E5%83%8F%E7%9A%84%E5%B0%BA%E5%AF%B8%EF%BC%8C%E6%8E%A5%E7%9D%80**%E6%97%8B%E8%BD%AC%E5%8D%B7%E7%A7%AF%E6%A0%B8**%EF%BC%8C%E5%86%8D%E8%BF%9B%E8%A1%8C**%E6%AD%A3%E5%90%91%E5%8D%B7%E7%A7%AF**%EF%BC%88padding%E5%8C%85%E5%90%ABstride%E5%8F%82%E6%95%B0%EF%BC%8C%E5%8D%B7%E7%A7%AF%E6%A0%B8%E4%B8%80%E8%88%AC%E5%A4%A7%E4%BA%8E2%5C*stride%EF%BC%8C%E4%B8%8B%E5%9B%BEstride%E7%AD%89%E4%BA%8E2%EF%BC%89%E3%80%82%0A!%5B9caf8f20d26dc5cc68aeec8ea3728eed.png%5D(en-resource%3A%2F%2Fdatabase%2F905%3A1)%0A%E7%9B%B8%E5%AF%B9upsample%EF%BC%8C%E8%83%BD%E8%8E%B7%E5%BE%97%E6%9B%B4%E5%A4%A7%E7%9A%84%E6%84%9F%E5%8F%97%E9%87%8Ereception%20field%0A%0A%23%23%23%23%203.%20keras.layers.Upsample2D%0A%E9%87%87%E7%94%A8%E7%9A%84%E6%98%AF%E4%B8%8A%E9%87%87%E6%A0%B7%E6%96%B9%E6%B3%95%0A%0A%0A%23%23%23%23%204.%20tensorflow.nn.conv2d_transpose%0A%E5%8F%8D%E5%8D%B7%E7%A7%AF</center></span>
</div></body></html> 