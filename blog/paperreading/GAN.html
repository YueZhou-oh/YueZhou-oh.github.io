<html>
<head>
  <title>Evernote Export</title>
  <basefont face="微软雅黑" size="2" />
  <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
  <meta name="exporter-version" content="YXBJ Windows/602628 (zh-CN, DDL); Windows/10.0.0 (Win64); EDAMVersion=V2;"/>
  <meta name="content-class" content="yinxiang.markdown"/>
  <style>
    body, td {
      font-family: 微软雅黑;
      font-size: 10pt;
    }
  </style>
</head>
<body>
<a name="832"/>

<div><span><div style="font-size: 14px; margin: 0; padding: 0; width: 100%;"><h3 style="line-height: 160%; box-sizing: content-box; font-weight: 700; font-size: 27px; color: #333;">笔记</h3>
<blockquote style="line-height: 160%; box-sizing: content-box; margin: 15px 0; border-left: 4px solid #ddd; padding: 0 15px; color: #777;">
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333; margin-top: 0; margin-bottom: 0;"><strong style="line-height: 160%; box-sizing: content-box; font-weight: 700;">详见GAN.pptx</strong></p>
</blockquote>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;"><img src="GAN_files/Image.png" type="image/png" data-filename="Image.png"/><br/>
<img src="GAN_files/Image [1].png" type="image/png" data-filename="Image.png"/></p>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;">诞生：GAN，Goodfellow，2014<br/>
引入卷积神经网络：DCGAN，2015-2016<br/>
其他流行架构：cGAN、StyleGAN、BigGAN、StackGAN、pix2pix、Age-cGAN、CycleGAN等<br/>
最新GAN跟进：https://github.com/hindupuravinash/the-gan-zoo</p>
<hr style="line-height: 160%; box-sizing: content-box; border-top: 1px solid #eee; margin: 16px 0;"/>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;"><a href="https://arxiv.org/abs/1312.6114" style="line-height: 160%; box-sizing: content-box; text-decoration: underline; color: #5286bc;">VAE</a>: Auto-Encoding Variational Bayes</p>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;"><a href="https://proceedings.neurips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf" style="line-height: 160%; box-sizing: content-box; text-decoration: underline; color: #5286bc;">GAN</a> : Generative Adversarial Networks</p>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;"><a href="https://arxiv.org/pdf/1511.06434.pdf" style="line-height: 160%; box-sizing: content-box; text-decoration: underline; color: #5286bc;">DCGAN</a> : Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks</p>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;"><a href="https://arxiv.org/abs/1411.1784" style="line-height: 160%; box-sizing: content-box; text-decoration: underline; color: #5286bc;">cGAN</a> : Conditional Generative Adversarial Nets</p>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;"><a href="https://arxiv.org/pdf/1812.04948.pdf" style="line-height: 160%; box-sizing: content-box; text-decoration: underline; color: #5286bc;">StyleGAN</a> : A Style-Based Generator Architecture for Generative Adversarial Network</p>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;"><a href="https://arxiv.org/abs/1809.11096" style="line-height: 160%; box-sizing: content-box; text-decoration: underline; color: #5286bc;">BigGAN</a> : Large Scale GAN Training for High Fidelity Natural Image Synthesis</p>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;"><a href="https://arxiv.org/abs/1612.03242" style="line-height: 160%; box-sizing: content-box; text-decoration: underline; color: #5286bc;">StackGAN</a> : StackGAN: Text to Photo-Realistic Image Synthesis with Stacked Generative Adversarial Networks</p>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;"><a href="https://arxiv.org/abs/1611.07004" style="line-height: 160%; box-sizing: content-box; text-decoration: underline; color: #5286bc;">Pix2pix</a> : Image-to-Image Translation with Conditional Adversarial Networks</p>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;"><a href="https://arxiv.org/abs/1702.01983" style="line-height: 160%; box-sizing: content-box; text-decoration: underline; color: #5286bc;">Age-cGAN</a> : Age Conditional Generative Adversarial Networks</p>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;"><a href="https://arxiv.org/pdf/1703.10593.pdf" style="line-height: 160%; box-sizing: content-box; text-decoration: underline; color: #5286bc;">CycleGAN</a> : Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks</p>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;"><a href="https://github.com/hindupuravinash/the-gan-zoo" style="line-height: 160%; box-sizing: content-box; text-decoration: underline; color: #5286bc;">The GAN Zoo</a> : Latest GANs updating</p>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;"><a style="line-height: 160%; box-sizing: content-box; text-decoration: underline; color: #5286bc;">MidiGAN</a></p>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;"><a style="line-height: 160%; box-sizing: content-box; text-decoration: underline; color: #5286bc;">AnoGAN</a></p>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;"><a style="line-height: 160%; box-sizing: content-box; text-decoration: underline; color: #5286bc;">SRGAN</a></p>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;"><a style="line-height: 160%; box-sizing: content-box; text-decoration: underline; color: #5286bc;">CosmoGAN</a></p>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;"><a style="line-height: 160%; box-sizing: content-box; text-decoration: underline; color: #5286bc;">MaskGAN</a></p>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;"><a style="line-height: 160%; box-sizing: content-box; text-decoration: underline; color: #5286bc;">DeblurGAN</a></p>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;"><a style="line-height: 160%; box-sizing: content-box; text-decoration: underline; color: #5286bc;">StarGAN</a></p>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;"><a href="https://arxiv.org/abs/1701.07875" style="line-height: 160%; box-sizing: content-box; text-decoration: underline; color: #5286bc;">WGAN</a> :改造loss function</p>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;"><a href="https://arxiv.org/abs/1704.00028" style="line-height: 160%; box-sizing: content-box; text-decoration: underline; color: #5286bc;">WGAN-GP</a>:中大正在实现，遇到问题</p>
<hr style="line-height: 160%; box-sizing: content-box; border-top: 1px solid #eee; margin: 16px 0;"/>
<h4 style="line-height: 160%; box-sizing: content-box; font-size: 20px; color: #333;">VAE</h4>
<blockquote style="line-height: 160%; box-sizing: content-box; margin: 15px 0; border-left: 4px solid #ddd; padding: 0 15px; color: #777;">
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333; margin-top: 0; margin-bottom: 0;">传统生成器，包含理论证明<br/>
用L1和L2 loss重建的图像很模糊，也就是说L1和L2并不能很好的恢复图像的高频部分(图像中的边缘等)，但能较好地恢复图像的低频部分(图像中的色块)。<br/>
输入为groudtruth，不是random latent vector</p>
</blockquote>
<h4 style="line-height: 160%; box-sizing: content-box; font-size: 20px; color: #333;">GAN</h4>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;">让<strong style="line-height: 160%; box-sizing: content-box; font-weight: 700;">generator和discriminator相互竞争</strong>（或合作，这是一个观点问题）。一个神经网络试图<strong style="line-height: 160%; box-sizing: content-box; font-weight: 700;">生成接近真实的数据</strong>（注意，GANs 可以用来模拟任何数据分布，但目前主要用于图像），另一个网络试图<strong style="line-height: 160%; box-sizing: content-box; font-weight: 700;">区分</strong>真实的数据和由生成网络生成的数据。</p>
<blockquote style="line-height: 160%; box-sizing: content-box; margin: 15px 0; border-left: 4px solid #ddd; padding: 0 15px; color: #777;">
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333; margin-top: 0; margin-bottom: 0;">重点阅读，包含理论证明，收敛性等</p>
</blockquote>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;"><img src="GAN_files/Image [2].png" type="image/png" data-filename="Image.png"/></p>
<h4 style="line-height: 160%; box-sizing: content-box; font-size: 20px; color: #333;">DCGAN</h4>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;"><strong style="line-height: 160%; box-sizing: content-box; font-weight: 700;">改进了模型的网络结构，设定了一些限制条件，工程上改进，使得训练更加稳定</strong>：</p>
<ul style="line-height: 160%; box-sizing: content-box; display: block; list-style-type: disc; padding-left: 30px; margin: 6px 0 10px; color: #333;">
<li style="line-height: 160%; box-sizing: content-box; position: relative;">用fractionally-strided convolutions代替pooling layer</li>
<li style="line-height: 160%; box-sizing: content-box; position: relative;">引入B.N.，except output of G &amp; input of D</li>
<li style="line-height: 160%; box-sizing: content-box; position: relative;">Remove fully-connected layer, except first layer of G</li>
<li style="line-height: 160%; box-sizing: content-box; position: relative;">In G, use tanh in output layer (saturate quicker) and ReLU in other layers</li>
<li style="line-height: 160%; box-sizing: content-box; position: relative;">In D, use LeakyReLU in all layers</li>
<li style="line-height: 160%; box-sizing: content-box; position: relative;">训练两次G &amp; 一次D,make sure that d_loss does not go to zero</li>
</ul>
<blockquote style="line-height: 160%; box-sizing: content-box; margin: 15px 0; border-left: 4px solid #ddd; padding: 0 15px; color: #777;">
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333; margin-top: 0; margin-bottom: 0;">更新D的时候不会更新G的参数，虽然是两个网络级联，但实际工程上，是分成两个网络，G输出是可以看成一个numpy变量输入到discriminator，并不是带参数的函数f(z)</p>
</blockquote>
<blockquote style="line-height: 160%; box-sizing: content-box; margin: 15px 0; border-left: 4px solid #ddd; padding: 0 15px; color: #777;">
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333; margin-top: 0; margin-bottom: 0;">重点阅读，官方实现，针对MNIST,给了cGAN实现</p>
</blockquote>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;"><img src="GAN_files/Image [3].png" type="image/png" data-filename="Image.png"/></p>
<h4 style="line-height: 160%; box-sizing: content-box; font-size: 20px; color: #333;">BigGAN</h4>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;">谷歌的实习生和谷歌DeepMind部门的两名研究人员发表, 这是GAN首次生成具有<strong style="line-height: 160%; box-sizing: content-box; font-weight: 700;">高保真度和低品种差距</strong>的图像。它最重要的改进是<strong style="line-height: 160%; box-sizing: content-box; font-weight: 700;">对生成器的正交正则化。</strong></p>
<h4 style="line-height: 160%; box-sizing: content-box; font-size: 20px; color: #333;">StyleGAN</h4>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;">NVIDIA发布，已迭代至StyleGAN2.<br/>
StyleGAN在<strong style="line-height: 160%; box-sizing: content-box; font-weight: 700;">面部生成</strong>任务中创造了新记录。算法的核心是<strong style="line-height: 160%; box-sizing: content-box; font-weight: 700;">风格转移技术或风格混合</strong>。除了生成面部外，它还可以<strong style="line-height: 160%; box-sizing: content-box; font-weight: 700;">生成高质量的汽车，卧室等图像</strong>。这是GANs领域的另一项重大改进，也是深度学习研究人员的灵感来源。</p>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;">FFHQ (Flickr-Faces-HQ) : 包含 7W 张1024*1024高清人脸照</p>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;">StyleGAN，提出了一个新的 generator architecture，能够控制所生成图像的高层级属性(high-level attributes)，如 发型、雀斑等<br/>
笔记：</p>
<ul style="line-height: 160%; box-sizing: content-box; display: block; list-style-type: disc; padding-left: 30px; margin: 6px 0 10px; color: #333;">
<li style="line-height: 160%; box-sizing: content-box; position: relative;">coarse spatial resolution：输出是4，8的G的模块，middle是16-32，fine style是64-1024</li>
<li style="line-height: 160%; box-sizing: content-box; position: relative;">A：affine transform，学习了两个变换，对同一输入，分别生成ys和yb，有待根据代码确认</li>
<li style="line-height: 160%; box-sizing: content-box; position: relative;">adaptive instance normalization，使用ys和yb</li>
<li style="line-height: 160%; box-sizing: content-box; position: relative;">style mixing，不同的resolution输入的style来自不同的latent vector</li>
<li style="line-height: 160%; box-sizing: content-box; position: relative;">usage of noise，应用在fine resolution使图像更真实</li>
<li style="line-height: 160%; box-sizing: content-box; position: relative;">loss function</li>
</ul>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;"><img src="GAN_files/Image [4].png" type="image/png" data-filename="Image.png"/></p>
<h4 style="line-height: 160%; box-sizing: content-box; font-size: 20px; color: #333;">StackGAN</h4>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;">使用StackGAN来<strong style="line-height: 160%; box-sizing: content-box; font-weight: 700;">探索文本到图像的合成</strong>，得到了非常好的结果。<strong style="line-height: 160%; box-sizing: content-box; font-weight: 700;">一个StackGAN由一对网络组成，当提供文本描述时，可以生成逼真的图像。</strong><br/>
Mainly for bird and flower</p>
<ul style="line-height: 160%; box-sizing: content-box; display: block; list-style-type: disc; padding-left: 30px; margin: 6px 0 10px; color: #333;">
<li style="line-height: 160%; box-sizing: content-box; position: relative;">CUB Dataset forbird<br/>
Caltech-UCSD Birds-200-2011 (CUB-200-2011) 是 CUB-200 dataset 的一个扩充版本，每个类的图像数量大约增加两倍和新的部位注释。（1）类别数目: 200<br/>
（2）图像总数目: 11,788<br/>
（3）每张图片的标注信息: 15 Part Locations, 312 Binary Attributes, 1 Bounding Box</li>
<li style="line-height: 160%; box-sizing: content-box; position: relative;">Oxford-102 dataset for flower<br/>
数据集由102类产自英国的花卉组成。每类由40-258张图片组成。</li>
<li style="line-height: 160%; box-sizing: content-box; position: relative;">MS COCO for other images</li>
</ul>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;">工程实现过程：</p>
<ul style="line-height: 160%; box-sizing: content-box; display: block; list-style-type: disc; padding-left: 30px; margin: 6px 0 10px; color: #333;">
<li style="line-height: 160%; box-sizing: content-box; position: relative;">相比GAN，1、引入CA，2、再往后叠加了stage2，即两个GAN</li>
<li style="line-height: 160%; box-sizing: content-box; position: relative;">G的输入text descriptor通过1.deep convolutional and recurrent text encoders[Learning deep representations for fine-grained visual descriptors]将image转化为text，然后2.通过word2vec转化为向量</li>
<li style="line-height: 160%; box-sizing: content-box; position: relative;">Conditioning augmentation的实际工程实现，1.假设c为100维，则CA即为连接200D的dense layer，前100维为mean，后100维为std，则经CA后的输出c为mean+epsilon*exp（std）；2.CA过程产生的loss也被叠加在generator loss上</li>
</ul>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;"><img src="GAN_files/Image [5].png" type="image/png" data-filename="Image.png"/></p>
<h4 style="line-height: 160%; box-sizing: content-box; font-size: 20px; color: #333;">pix2pix</h4>
<blockquote style="line-height: 160%; box-sizing: content-box; margin: 15px 0; border-left: 4px solid #ddd; padding: 0 15px; color: #777;">
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333; margin-top: 0; margin-bottom: 0;">其实可以看成是有监督的，因为输入的是轮廓，可以看成是对应图像的label<br/>
对于<strong style="line-height: 160%; box-sizing: content-box; font-weight: 700;">图像到图像的翻译任务</strong>，pix2pix也显示出了令人印象深刻的结果。无论是<strong style="line-height: 160%; box-sizing: content-box; font-weight: 700;">将夜间图像转换为白天的图像还是给黑白图像着色，或者将草图转换为逼真的照片</strong>等等，Pix2pix在这些例子中都表现非常出色。</p>
</blockquote>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;">GAN其实是一种相对于L1 loss更好的判别准则或者loss。有时候单独使用GAN loss效果好，有时候与L1 loss配合起来效果好。在pix2pix中，作者就是把L1 loss 和GAN loss相结合使用，因为<strong style="line-height: 160%; box-sizing: content-box; font-weight: 700;">L1 loss 可以恢复图像的低频部分，而GAN loss可以恢复图像的高频部分。</strong><br/>
缺点：使用这样的结构其实学到的是x到y之间的一对一映射！也就说，pix2pix就是对ground truth的重建：输入轮廓图→经过Unet编码解码成对应的向量→解码成真实图。例如，当我们输入训练集中不存在的轮廓图时，重建效果不ok</p>
<ul style="line-height: 160%; box-sizing: content-box; display: block; list-style-type: disc; padding-left: 30px; margin: 6px 0 10px; color: #333;">
<li style="line-height: 160%; box-sizing: content-box; position: relative;">generator: Unet</li>
<li style="line-height: 160%; box-sizing: content-box; position: relative;">discriminitor: proposed <strong style="line-height: 160%; box-sizing: content-box; font-weight: 700;">patchGAN</strong>, 把图像<strong style="line-height: 160%; box-sizing: content-box; font-weight: 700;">等分成patch</strong>，分别判断每个Patch的真假，最后再<strong style="line-height: 160%; box-sizing: content-box; font-weight: 700;">取平均！<strong style="line-height: 160%; box-sizing: content-box; font-weight: 700;">作者最后说，文章提出的这个PatchGAN可以看成所以</strong>另一种形式的纹理损失或样式损失</strong>。在具体实验时，不同尺寸的patch，最后发现<strong style="line-height: 160%; box-sizing: content-box; font-weight: 700;">70x70的尺寸比较合适</strong>。</li>
<li style="line-height: 160%; box-sizing: content-box; position: relative;">对比有无L1 loss时生成图像的区别</li>
</ul>
<h4 style="line-height: 160%; box-sizing: content-box; font-size: 20px; color: #333;">CycleGAN，2017</h4>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;">CycleGAN有一些非常有趣的用例，例如<strong style="line-height: 160%; box-sizing: content-box; font-weight: 700;">将照片转换为绘画</strong>，将夏季拍摄的照片转换为冬季拍摄的照片，或将马的照片转换为斑马照片，或者相反。CycleGAN<strong style="line-height: 160%; box-sizing: content-box; font-weight: 700;">用于不同的图像到图像翻译</strong>。</p>
<blockquote style="line-height: 160%; box-sizing: content-box; margin: 15px 0; border-left: 4px solid #ddd; padding: 0 15px; color: #777;">
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333; margin-top: 0; margin-bottom: 0;">pix2pix是paired，cyclegan是unpaired<br/>
input is not random latent vector<br/>
instance normalization &amp; patchGAN<br/>
四个loss function</p>
</blockquote>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;"><img src="GAN_files/Image [6].png" type="image/png" data-filename="Image.png"/><br/>
<img src="GAN_files/Image [7].png" type="image/png" data-filename="Image.png"/></p>
<h4 style="line-height: 160%; box-sizing: content-box; font-size: 20px; color: #333;">cGAN</h4>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;">紧随着原生GAN出现的。在这篇文章中，作者在<strong style="line-height: 160%; box-sizing: content-box; font-weight: 700;">输入的时候加入了条件</strong>（类别标签或者其他模态的信息），比如在MNIST训练好的网络，可以<strong style="line-height: 160%; box-sizing: content-box; font-weight: 700;">指定生成某一个具体数字的图像，这就成了有监督的GAN</strong>。同时，在文章中，作者还使用网络进行了<strong style="line-height: 160%; box-sizing: content-box; font-weight: 700;">图像自动标注</strong>。</p>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;">例如MNIST，input vector=[random latent vector] + [class label vector]<br/>
在原生GAN中，判别器的输入是训练样本x，生成器的的输入是噪声z，在conditional GAN中，生成器和判别器的输入都多了一个y，这个y就是那个条件。以手写字符数据集MNIST为例，这时候x代表图片向量，y代表图片类别对应的label(one-hot表示的0~9)。<br/>
<img src="GAN_files/Image [8].png" type="image/png" data-filename="Image.png"/></p>
<h4 style="line-height: 160%; box-sizing: content-box; font-size: 20px; color: #333;">Age-cGAN</h4>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;">面部老化有许多行业用例，包括跨年龄人脸识别，寻找失踪儿童，或者用于娱乐。论文中提出了<strong style="line-height: 160%; box-sizing: content-box; font-weight: 700;">用条件GAN进行面部老化</strong>。</p>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;">基于latent vector&amp;generator&amp;discriminator的变形，多次的实验尝试，找到最佳结构，给予网络结构的为何能实现指定应用的合理解释</p>
<ul style="line-height: 160%; box-sizing: content-box; display: block; list-style-type: disc; padding-left: 30px; margin: 6px 0 10px; color: #333;">
<li style="line-height: 160%; box-sizing: content-box; position: relative;">condition is a vector represent age</li>
<li style="line-height: 160%; box-sizing: content-box; position: relative;">modify to generator</li>
</ul>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;"><img src="GAN_files/Image [9].png" type="image/png" data-filename="Image.png"/></p>
</div><center style="display:none !important;visibility:collapse !important;height:0 !important;white-space:nowrap;width:100%;overflow:hidden">%23%23%23%20%E7%AC%94%E8%AE%B0%0A%3E%20**%E8%AF%A6%E8%A7%81GAN.pptx**%0A%0A!%5B04386b5bb8df073718b6d1231f539d14.png%5D(en-resource%3A%2F%2Fdatabase%2F834%3A1)%0A!%5B4151886217dc2ca46afd113bb6ac3e77.png%5D(en-resource%3A%2F%2Fdatabase%2F836%3A1)%0A%0A%0A%E8%AF%9E%E7%94%9F%EF%BC%9AGAN%EF%BC%8CGoodfellow%EF%BC%8C2014%0A%E5%BC%95%E5%85%A5%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9ADCGAN%EF%BC%8C2015-2016%0A%E5%85%B6%E4%BB%96%E6%B5%81%E8%A1%8C%E6%9E%B6%E6%9E%84%EF%BC%9AcGAN%E3%80%81StyleGAN%E3%80%81BigGAN%E3%80%81StackGAN%E3%80%81pix2pix%E3%80%81Age-cGAN%E3%80%81CycleGAN%E7%AD%89%0A%E6%9C%80%E6%96%B0GAN%E8%B7%9F%E8%BF%9B%EF%BC%9Ahttps%3A%2F%2Fgithub.com%2Fhindupuravinash%2Fthe-gan-zoo%0A%0A---------%0A%5BVAE%5D(https%3A%2F%2Farxiv.org%2Fabs%2F1312.6114)%3A%20Auto-Encoding%20Variational%20Bayes%0A%5BGAN%5D(https%3A%2F%2Fproceedings.neurips.cc%2Fpaper%2F2014%2Ffile%2F5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf)%20%3A%20Generative%20Adversarial%20Networks%0A%5BDCGAN%5D(https%3A%2F%2Farxiv.org%2Fpdf%2F1511.06434.pdf)%20%3A%20Unsupervised%20Representation%20Learning%20with%20Deep%20Convolutional%20Generative%20Adversarial%20Networks%0A%5BcGAN%5D(https%3A%2F%2Farxiv.org%2Fabs%2F1411.1784)%20%3A%20Conditional%20Generative%20Adversarial%20Nets%0A%5BStyleGAN%5D(https%3A%2F%2Farxiv.org%2Fpdf%2F1812.04948.pdf)%20%3A%20A%20Style-Based%20Generator%20Architecture%20for%20Generative%20Adversarial%20Network%0A%5BBigGAN%5D(https%3A%2F%2Farxiv.org%2Fabs%2F1809.11096)%20%3A%20Large%20Scale%20GAN%20Training%20for%20High%20Fidelity%20Natural%20Image%20Synthesis%0A%5BStackGAN%5D(https%3A%2F%2Farxiv.org%2Fabs%2F1612.03242)%20%3A%20StackGAN%3A%20Text%20to%20Photo-Realistic%20Image%20Synthesis%20with%20Stacked%20Generative%20Adversarial%20Networks%0A%5BPix2pix%5D(https%3A%2F%2Farxiv.org%2Fabs%2F1611.07004)%20%3A%20Image-to-Image%20Translation%20with%20Conditional%20Adversarial%20Networks%0A%5BAge-cGAN%5D(https%3A%2F%2Farxiv.org%2Fabs%2F1702.01983)%20%3A%20Age%20Conditional%20Generative%20Adversarial%20Networks%0A%5BCycleGAN%5D(https%3A%2F%2Farxiv.org%2Fpdf%2F1703.10593.pdf)%20%3A%20Unpaired%20Image-to-Image%20Translation%20using%20Cycle-Consistent%20Adversarial%20Networks%0A%5BThe%20GAN%20Zoo%5D(https%3A%2F%2Fgithub.com%2Fhindupuravinash%2Fthe-gan-zoo)%20%3A%20Latest%20GANs%20updating%0A%5BMidiGAN%5D()%0A%5BAnoGAN%5D()%0A%5BSRGAN%5D()%0A%5BCosmoGAN%5D()%0A%5BMaskGAN%5D()%0A%5BDeblurGAN%5D()%0A%5BStarGAN%5D()%0A%5BWGAN%5D(https%3A%2F%2Farxiv.org%2Fabs%2F1701.07875)%20%3A%E6%94%B9%E9%80%A0loss%20function%0A%5BWGAN-GP%5D(https%3A%2F%2Farxiv.org%2Fabs%2F1704.00028)%3A%E4%B8%AD%E5%A4%A7%E6%AD%A3%E5%9C%A8%E5%AE%9E%E7%8E%B0%EF%BC%8C%E9%81%87%E5%88%B0%E9%97%AE%E9%A2%98%0A%0A-----------------%0A%23%23%23%23%20VAE%0A%3E%E4%BC%A0%E7%BB%9F%E7%94%9F%E6%88%90%E5%99%A8%EF%BC%8C%E5%8C%85%E5%90%AB%E7%90%86%E8%AE%BA%E8%AF%81%E6%98%8E%0A%3E%E7%94%A8L1%E5%92%8CL2%20loss%E9%87%8D%E5%BB%BA%E7%9A%84%E5%9B%BE%E5%83%8F%E5%BE%88%E6%A8%A1%E7%B3%8A%EF%BC%8C%E4%B9%9F%E5%B0%B1%E6%98%AF%E8%AF%B4L1%E5%92%8CL2%E5%B9%B6%E4%B8%8D%E8%83%BD%E5%BE%88%E5%A5%BD%E7%9A%84%E6%81%A2%E5%A4%8D%E5%9B%BE%E5%83%8F%E7%9A%84%E9%AB%98%E9%A2%91%E9%83%A8%E5%88%86(%E5%9B%BE%E5%83%8F%E4%B8%AD%E7%9A%84%E8%BE%B9%E7%BC%98%E7%AD%89)%EF%BC%8C%E4%BD%86%E8%83%BD%E8%BE%83%E5%A5%BD%E5%9C%B0%E6%81%A2%E5%A4%8D%E5%9B%BE%E5%83%8F%E7%9A%84%E4%BD%8E%E9%A2%91%E9%83%A8%E5%88%86(%E5%9B%BE%E5%83%8F%E4%B8%AD%E7%9A%84%E8%89%B2%E5%9D%97)%E3%80%82%0A%3E%E8%BE%93%E5%85%A5%E4%B8%BAgroudtruth%EF%BC%8C%E4%B8%8D%E6%98%AFrandom%20latent%20vector%0A%0A%23%23%23%23%20GAN%0A%E8%AE%A9**generator%E5%92%8Cdiscriminator%E7%9B%B8%E4%BA%92%E7%AB%9E%E4%BA%89**%EF%BC%88%E6%88%96%E5%90%88%E4%BD%9C%EF%BC%8C%E8%BF%99%E6%98%AF%E4%B8%80%E4%B8%AA%E8%A7%82%E7%82%B9%E9%97%AE%E9%A2%98%EF%BC%89%E3%80%82%E4%B8%80%E4%B8%AA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AF%95%E5%9B%BE**%E7%94%9F%E6%88%90%E6%8E%A5%E8%BF%91%E7%9C%9F%E5%AE%9E%E7%9A%84%E6%95%B0%E6%8D%AE**%EF%BC%88%E6%B3%A8%E6%84%8F%EF%BC%8CGANs%20%E5%8F%AF%E4%BB%A5%E7%94%A8%E6%9D%A5%E6%A8%A1%E6%8B%9F%E4%BB%BB%E4%BD%95%E6%95%B0%E6%8D%AE%E5%88%86%E5%B8%83%EF%BC%8C%E4%BD%86%E7%9B%AE%E5%89%8D%E4%B8%BB%E8%A6%81%E7%94%A8%E4%BA%8E%E5%9B%BE%E5%83%8F%EF%BC%89%EF%BC%8C%E5%8F%A6%E4%B8%80%E4%B8%AA%E7%BD%91%E7%BB%9C%E8%AF%95%E5%9B%BE**%E5%8C%BA%E5%88%86**%E7%9C%9F%E5%AE%9E%E7%9A%84%E6%95%B0%E6%8D%AE%E5%92%8C%E7%94%B1%E7%94%9F%E6%88%90%E7%BD%91%E7%BB%9C%E7%94%9F%E6%88%90%E7%9A%84%E6%95%B0%E6%8D%AE%E3%80%82%0A%3E%E9%87%8D%E7%82%B9%E9%98%85%E8%AF%BB%EF%BC%8C%E5%8C%85%E5%90%AB%E7%90%86%E8%AE%BA%E8%AF%81%E6%98%8E%EF%BC%8C%E6%94%B6%E6%95%9B%E6%80%A7%E7%AD%89%0A%0A!%5B2b0857af6b14357e930d2e8dd273b750.png%5D(en-resource%3A%2F%2Fdatabase%2F1041%3A1)%0A%0A%23%23%23%23%20DCGAN%0A**%E6%94%B9%E8%BF%9B%E4%BA%86%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%EF%BC%8C%E8%AE%BE%E5%AE%9A%E4%BA%86%E4%B8%80%E4%BA%9B%E9%99%90%E5%88%B6%E6%9D%A1%E4%BB%B6%EF%BC%8C%E5%B7%A5%E7%A8%8B%E4%B8%8A%E6%94%B9%E8%BF%9B%EF%BC%8C%E4%BD%BF%E5%BE%97%E8%AE%AD%E7%BB%83%E6%9B%B4%E5%8A%A0%E7%A8%B3%E5%AE%9A**%EF%BC%9A%0A-%20%E7%94%A8fractionally-strided%20convolutions%E4%BB%A3%E6%9B%BFpooling%20layer%0A-%20%E5%BC%95%E5%85%A5B.N.%EF%BC%8Cexcept%20output%20of%20G%20%26%20input%20of%20D%0A-%20Remove%20fully-connected%20layer%2C%20except%20first%20layer%20of%20G%0A-%20In%20G%2C%20use%20tanh%20in%20output%20layer%20(saturate%20quicker)%20and%20ReLU%20in%20other%20layers%0A-%20In%20D%2C%20use%20LeakyReLU%20in%20all%20layers%0A-%20%E8%AE%AD%E7%BB%83%E4%B8%A4%E6%AC%A1G%20%26%20%E4%B8%80%E6%AC%A1D%2Cmake%20sure%20that%20d_loss%20does%20not%20go%20to%20zero%0A%0A%3E%E6%9B%B4%E6%96%B0D%E7%9A%84%E6%97%B6%E5%80%99%E4%B8%8D%E4%BC%9A%E6%9B%B4%E6%96%B0G%E7%9A%84%E5%8F%82%E6%95%B0%EF%BC%8C%E8%99%BD%E7%84%B6%E6%98%AF%E4%B8%A4%E4%B8%AA%E7%BD%91%E7%BB%9C%E7%BA%A7%E8%81%94%EF%BC%8C%E4%BD%86%E5%AE%9E%E9%99%85%E5%B7%A5%E7%A8%8B%E4%B8%8A%EF%BC%8C%E6%98%AF%E5%88%86%E6%88%90%E4%B8%A4%E4%B8%AA%E7%BD%91%E7%BB%9C%EF%BC%8CG%E8%BE%93%E5%87%BA%E6%98%AF%E5%8F%AF%E4%BB%A5%E7%9C%8B%E6%88%90%E4%B8%80%E4%B8%AAnumpy%E5%8F%98%E9%87%8F%E8%BE%93%E5%85%A5%E5%88%B0discriminator%EF%BC%8C%E5%B9%B6%E4%B8%8D%E6%98%AF%E5%B8%A6%E5%8F%82%E6%95%B0%E7%9A%84%E5%87%BD%E6%95%B0f(z)%0A%0A%3E%E9%87%8D%E7%82%B9%E9%98%85%E8%AF%BB%EF%BC%8C%E5%AE%98%E6%96%B9%E5%AE%9E%E7%8E%B0%EF%BC%8C%E9%92%88%E5%AF%B9MNIST%2C%E7%BB%99%E4%BA%86cGAN%E5%AE%9E%E7%8E%B0%0A%0A!%5B2be2869af4b6cf5a7e90919e35869a1f.png%5D(en-resource%3A%2F%2Fdatabase%2F1027%3A1)%0A%0A%23%23%23%23%20BigGAN%0A%E8%B0%B7%E6%AD%8C%E7%9A%84%E5%AE%9E%E4%B9%A0%E7%94%9F%E5%92%8C%E8%B0%B7%E6%AD%8CDeepMind%E9%83%A8%E9%97%A8%E7%9A%84%E4%B8%A4%E5%90%8D%E7%A0%94%E7%A9%B6%E4%BA%BA%E5%91%98%E5%8F%91%E8%A1%A8%2C%20%E8%BF%99%E6%98%AFGAN%E9%A6%96%E6%AC%A1%E7%94%9F%E6%88%90%E5%85%B7%E6%9C%89**%E9%AB%98%E4%BF%9D%E7%9C%9F%E5%BA%A6%E5%92%8C%E4%BD%8E%E5%93%81%E7%A7%8D%E5%B7%AE%E8%B7%9D**%E7%9A%84%E5%9B%BE%E5%83%8F%E3%80%82%E5%AE%83%E6%9C%80%E9%87%8D%E8%A6%81%E7%9A%84%E6%94%B9%E8%BF%9B%E6%98%AF**%E5%AF%B9%E7%94%9F%E6%88%90%E5%99%A8%E7%9A%84%E6%AD%A3%E4%BA%A4%E6%AD%A3%E5%88%99%E5%8C%96%E3%80%82**%0A%0A%23%23%23%23%20StyleGAN%0ANVIDIA%E5%8F%91%E5%B8%83%EF%BC%8C%E5%B7%B2%E8%BF%AD%E4%BB%A3%E8%87%B3StyleGAN2.%0AStyleGAN%E5%9C%A8**%E9%9D%A2%E9%83%A8%E7%94%9F%E6%88%90**%E4%BB%BB%E5%8A%A1%E4%B8%AD%E5%88%9B%E9%80%A0%E4%BA%86%E6%96%B0%E8%AE%B0%E5%BD%95%E3%80%82%E7%AE%97%E6%B3%95%E7%9A%84%E6%A0%B8%E5%BF%83%E6%98%AF**%E9%A3%8E%E6%A0%BC%E8%BD%AC%E7%A7%BB%E6%8A%80%E6%9C%AF%E6%88%96%E9%A3%8E%E6%A0%BC%E6%B7%B7%E5%90%88**%E3%80%82%E9%99%A4%E4%BA%86%E7%94%9F%E6%88%90%E9%9D%A2%E9%83%A8%E5%A4%96%EF%BC%8C%E5%AE%83%E8%BF%98%E5%8F%AF%E4%BB%A5**%E7%94%9F%E6%88%90%E9%AB%98%E8%B4%A8%E9%87%8F%E7%9A%84%E6%B1%BD%E8%BD%A6%EF%BC%8C%E5%8D%A7%E5%AE%A4%E7%AD%89%E5%9B%BE%E5%83%8F**%E3%80%82%E8%BF%99%E6%98%AFGANs%E9%A2%86%E5%9F%9F%E7%9A%84%E5%8F%A6%E4%B8%80%E9%A1%B9%E9%87%8D%E5%A4%A7%E6%94%B9%E8%BF%9B%EF%BC%8C%E4%B9%9F%E6%98%AF%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%A0%94%E7%A9%B6%E4%BA%BA%E5%91%98%E7%9A%84%E7%81%B5%E6%84%9F%E6%9D%A5%E6%BA%90%E3%80%82%0A%0AFFHQ%20(Flickr-Faces-HQ)%20%3A%20%E5%8C%85%E5%90%AB%207W%20%E5%BC%A01024%5C*1024%E9%AB%98%E6%B8%85%E4%BA%BA%E8%84%B8%E7%85%A7%0A%0AStyleGAN%EF%BC%8C%E6%8F%90%E5%87%BA%E4%BA%86%E4%B8%80%E4%B8%AA%E6%96%B0%E7%9A%84%20generator%20architecture%EF%BC%8C%E8%83%BD%E5%A4%9F%E6%8E%A7%E5%88%B6%E6%89%80%E7%94%9F%E6%88%90%E5%9B%BE%E5%83%8F%E7%9A%84%E9%AB%98%E5%B1%82%E7%BA%A7%E5%B1%9E%E6%80%A7(high-level%20attributes)%EF%BC%8C%E5%A6%82%20%E5%8F%91%E5%9E%8B%E3%80%81%E9%9B%80%E6%96%91%E7%AD%89%0A%E7%AC%94%E8%AE%B0%EF%BC%9A%0A-%20coarse%20spatial%20resolution%EF%BC%9A%E8%BE%93%E5%87%BA%E6%98%AF4%EF%BC%8C8%E7%9A%84G%E7%9A%84%E6%A8%A1%E5%9D%97%EF%BC%8Cmiddle%E6%98%AF16-32%EF%BC%8Cfine%20style%E6%98%AF64-1024%0A-%20A%EF%BC%9Aaffine%20transform%EF%BC%8C%E5%AD%A6%E4%B9%A0%E4%BA%86%E4%B8%A4%E4%B8%AA%E5%8F%98%E6%8D%A2%EF%BC%8C%E5%AF%B9%E5%90%8C%E4%B8%80%E8%BE%93%E5%85%A5%EF%BC%8C%E5%88%86%E5%88%AB%E7%94%9F%E6%88%90ys%E5%92%8Cyb%EF%BC%8C%E6%9C%89%E5%BE%85%E6%A0%B9%E6%8D%AE%E4%BB%A3%E7%A0%81%E7%A1%AE%E8%AE%A4%0A-%20adaptive%20instance%20normalization%EF%BC%8C%E4%BD%BF%E7%94%A8ys%E5%92%8Cyb%0A-%20style%20mixing%EF%BC%8C%E4%B8%8D%E5%90%8C%E7%9A%84resolution%E8%BE%93%E5%85%A5%E7%9A%84style%E6%9D%A5%E8%87%AA%E4%B8%8D%E5%90%8C%E7%9A%84latent%20vector%0A-%20usage%20of%20noise%EF%BC%8C%E5%BA%94%E7%94%A8%E5%9C%A8fine%20resolution%E4%BD%BF%E5%9B%BE%E5%83%8F%E6%9B%B4%E7%9C%9F%E5%AE%9E%0A-%20loss%20function%0A%0A!%5Bcfde04367ba59ce69ccfcf1bb6d7f8e9.png%5D(en-resource%3A%2F%2Fdatabase%2F1025%3A1)%0A%0A%23%23%23%23%20StackGAN%0A%E4%BD%BF%E7%94%A8StackGAN%E6%9D%A5**%E6%8E%A2%E7%B4%A2%E6%96%87%E6%9C%AC%E5%88%B0%E5%9B%BE%E5%83%8F%E7%9A%84%E5%90%88%E6%88%90**%EF%BC%8C%E5%BE%97%E5%88%B0%E4%BA%86%E9%9D%9E%E5%B8%B8%E5%A5%BD%E7%9A%84%E7%BB%93%E6%9E%9C%E3%80%82**%E4%B8%80%E4%B8%AAStackGAN%E7%94%B1%E4%B8%80%E5%AF%B9%E7%BD%91%E7%BB%9C%E7%BB%84%E6%88%90%EF%BC%8C%E5%BD%93%E6%8F%90%E4%BE%9B%E6%96%87%E6%9C%AC%E6%8F%8F%E8%BF%B0%E6%97%B6%EF%BC%8C%E5%8F%AF%E4%BB%A5%E7%94%9F%E6%88%90%E9%80%BC%E7%9C%9F%E7%9A%84%E5%9B%BE%E5%83%8F%E3%80%82**%0AMainly%20for%20bird%20and%20flower%0A-%20CUB%20Dataset%20forbird%0ACaltech-UCSD%20Birds-200-2011%20(CUB-200-2011)%20%E6%98%AF%20CUB-200%20dataset%20%E7%9A%84%E4%B8%80%E4%B8%AA%E6%89%A9%E5%85%85%E7%89%88%E6%9C%AC%EF%BC%8C%E6%AF%8F%E4%B8%AA%E7%B1%BB%E7%9A%84%E5%9B%BE%E5%83%8F%E6%95%B0%E9%87%8F%E5%A4%A7%E7%BA%A6%E5%A2%9E%E5%8A%A0%E4%B8%A4%E5%80%8D%E5%92%8C%E6%96%B0%E7%9A%84%E9%83%A8%E4%BD%8D%E6%B3%A8%E9%87%8A%E3%80%82%EF%BC%881%EF%BC%89%E7%B1%BB%E5%88%AB%E6%95%B0%E7%9B%AE%3A%20200%0A%EF%BC%882%EF%BC%89%E5%9B%BE%E5%83%8F%E6%80%BB%E6%95%B0%E7%9B%AE%3A%2011%2C788%0A%EF%BC%883%EF%BC%89%E6%AF%8F%E5%BC%A0%E5%9B%BE%E7%89%87%E7%9A%84%E6%A0%87%E6%B3%A8%E4%BF%A1%E6%81%AF%3A%2015%20Part%20Locations%2C%20312%20Binary%20Attributes%2C%201%20Bounding%20Box%0A-%20Oxford-102%20dataset%20for%20flower%0A%E6%95%B0%E6%8D%AE%E9%9B%86%E7%94%B1102%E7%B1%BB%E4%BA%A7%E8%87%AA%E8%8B%B1%E5%9B%BD%E7%9A%84%E8%8A%B1%E5%8D%89%E7%BB%84%E6%88%90%E3%80%82%E6%AF%8F%E7%B1%BB%E7%94%B140-258%E5%BC%A0%E5%9B%BE%E7%89%87%E7%BB%84%E6%88%90%E3%80%82%0A-%20MS%20COCO%20for%20other%20images%0A%0A%E5%B7%A5%E7%A8%8B%E5%AE%9E%E7%8E%B0%E8%BF%87%E7%A8%8B%EF%BC%9A%0A-%20%E7%9B%B8%E6%AF%94GAN%EF%BC%8C1%E3%80%81%E5%BC%95%E5%85%A5CA%EF%BC%8C2%E3%80%81%E5%86%8D%E5%BE%80%E5%90%8E%E5%8F%A0%E5%8A%A0%E4%BA%86stage2%EF%BC%8C%E5%8D%B3%E4%B8%A4%E4%B8%AAGAN%0A-%20G%E7%9A%84%E8%BE%93%E5%85%A5text%20descriptor%E9%80%9A%E8%BF%871.deep%20convolutional%20and%20recurrent%20text%20encoders%5BLearning%20deep%20representations%20for%20fine-grained%20visual%20descriptors%5D%E5%B0%86image%E8%BD%AC%E5%8C%96%E4%B8%BAtext%EF%BC%8C%E7%84%B6%E5%90%8E2.%E9%80%9A%E8%BF%87word2vec%E8%BD%AC%E5%8C%96%E4%B8%BA%E5%90%91%E9%87%8F%0A-%20Conditioning%20augmentation%E7%9A%84%E5%AE%9E%E9%99%85%E5%B7%A5%E7%A8%8B%E5%AE%9E%E7%8E%B0%EF%BC%8C1.%E5%81%87%E8%AE%BEc%E4%B8%BA100%E7%BB%B4%EF%BC%8C%E5%88%99CA%E5%8D%B3%E4%B8%BA%E8%BF%9E%E6%8E%A5200D%E7%9A%84dense%20layer%EF%BC%8C%E5%89%8D100%E7%BB%B4%E4%B8%BAmean%EF%BC%8C%E5%90%8E100%E7%BB%B4%E4%B8%BAstd%EF%BC%8C%E5%88%99%E7%BB%8FCA%E5%90%8E%E7%9A%84%E8%BE%93%E5%87%BAc%E4%B8%BAmean%2Bepsilon%5C*exp%EF%BC%88std%EF%BC%89%EF%BC%9B2.CA%E8%BF%87%E7%A8%8B%E4%BA%A7%E7%94%9F%E7%9A%84loss%E4%B9%9F%E8%A2%AB%E5%8F%A0%E5%8A%A0%E5%9C%A8generator%20loss%E4%B8%8A%0A%0A!%5Bcc05b498dc14d7e84b0c9402df543127.png%5D(en-resource%3A%2F%2Fdatabase%2F1043%3A1)%0A%0A%0A%23%23%23%23%20pix2pix%0A%3E%E5%85%B6%E5%AE%9E%E5%8F%AF%E4%BB%A5%E7%9C%8B%E6%88%90%E6%98%AF%E6%9C%89%E7%9B%91%E7%9D%A3%E7%9A%84%EF%BC%8C%E5%9B%A0%E4%B8%BA%E8%BE%93%E5%85%A5%E7%9A%84%E6%98%AF%E8%BD%AE%E5%BB%93%EF%BC%8C%E5%8F%AF%E4%BB%A5%E7%9C%8B%E6%88%90%E6%98%AF%E5%AF%B9%E5%BA%94%E5%9B%BE%E5%83%8F%E7%9A%84label%0A%E5%AF%B9%E4%BA%8E**%E5%9B%BE%E5%83%8F%E5%88%B0%E5%9B%BE%E5%83%8F%E7%9A%84%E7%BF%BB%E8%AF%91%E4%BB%BB%E5%8A%A1**%EF%BC%8Cpix2pix%E4%B9%9F%E6%98%BE%E7%A4%BA%E5%87%BA%E4%BA%86%E4%BB%A4%E4%BA%BA%E5%8D%B0%E8%B1%A1%E6%B7%B1%E5%88%BB%E7%9A%84%E7%BB%93%E6%9E%9C%E3%80%82%E6%97%A0%E8%AE%BA%E6%98%AF**%E5%B0%86%E5%A4%9C%E9%97%B4%E5%9B%BE%E5%83%8F%E8%BD%AC%E6%8D%A2%E4%B8%BA%E7%99%BD%E5%A4%A9%E7%9A%84%E5%9B%BE%E5%83%8F%E8%BF%98%E6%98%AF%E7%BB%99%E9%BB%91%E7%99%BD%E5%9B%BE%E5%83%8F%E7%9D%80%E8%89%B2%EF%BC%8C%E6%88%96%E8%80%85%E5%B0%86%E8%8D%89%E5%9B%BE%E8%BD%AC%E6%8D%A2%E4%B8%BA%E9%80%BC%E7%9C%9F%E7%9A%84%E7%85%A7%E7%89%87**%E7%AD%89%E7%AD%89%EF%BC%8CPix2pix%E5%9C%A8%E8%BF%99%E4%BA%9B%E4%BE%8B%E5%AD%90%E4%B8%AD%E9%83%BD%E8%A1%A8%E7%8E%B0%E9%9D%9E%E5%B8%B8%E5%87%BA%E8%89%B2%E3%80%82%0A%0AGAN%E5%85%B6%E5%AE%9E%E6%98%AF%E4%B8%80%E7%A7%8D%E7%9B%B8%E5%AF%B9%E4%BA%8EL1%20loss%E6%9B%B4%E5%A5%BD%E7%9A%84%E5%88%A4%E5%88%AB%E5%87%86%E5%88%99%E6%88%96%E8%80%85loss%E3%80%82%E6%9C%89%E6%97%B6%E5%80%99%E5%8D%95%E7%8B%AC%E4%BD%BF%E7%94%A8GAN%20loss%E6%95%88%E6%9E%9C%E5%A5%BD%EF%BC%8C%E6%9C%89%E6%97%B6%E5%80%99%E4%B8%8EL1%20loss%E9%85%8D%E5%90%88%E8%B5%B7%E6%9D%A5%E6%95%88%E6%9E%9C%E5%A5%BD%E3%80%82%E5%9C%A8pix2pix%E4%B8%AD%EF%BC%8C%E4%BD%9C%E8%80%85%E5%B0%B1%E6%98%AF%E6%8A%8AL1%20loss%20%E5%92%8CGAN%20loss%E7%9B%B8%E7%BB%93%E5%90%88%E4%BD%BF%E7%94%A8%EF%BC%8C%E5%9B%A0%E4%B8%BA**L1%20loss%20%E5%8F%AF%E4%BB%A5%E6%81%A2%E5%A4%8D%E5%9B%BE%E5%83%8F%E7%9A%84%E4%BD%8E%E9%A2%91%E9%83%A8%E5%88%86%EF%BC%8C%E8%80%8CGAN%20loss%E5%8F%AF%E4%BB%A5%E6%81%A2%E5%A4%8D%E5%9B%BE%E5%83%8F%E7%9A%84%E9%AB%98%E9%A2%91%E9%83%A8%E5%88%86%E3%80%82**%0A%E7%BC%BA%E7%82%B9%EF%BC%9A%E4%BD%BF%E7%94%A8%E8%BF%99%E6%A0%B7%E7%9A%84%E7%BB%93%E6%9E%84%E5%85%B6%E5%AE%9E%E5%AD%A6%E5%88%B0%E7%9A%84%E6%98%AFx%E5%88%B0y%E4%B9%8B%E9%97%B4%E7%9A%84%E4%B8%80%E5%AF%B9%E4%B8%80%E6%98%A0%E5%B0%84%EF%BC%81%E4%B9%9F%E5%B0%B1%E8%AF%B4%EF%BC%8Cpix2pix%E5%B0%B1%E6%98%AF%E5%AF%B9ground%20truth%E7%9A%84%E9%87%8D%E5%BB%BA%EF%BC%9A%E8%BE%93%E5%85%A5%E8%BD%AE%E5%BB%93%E5%9B%BE%E2%86%92%E7%BB%8F%E8%BF%87Unet%E7%BC%96%E7%A0%81%E8%A7%A3%E7%A0%81%E6%88%90%E5%AF%B9%E5%BA%94%E7%9A%84%E5%90%91%E9%87%8F%E2%86%92%E8%A7%A3%E7%A0%81%E6%88%90%E7%9C%9F%E5%AE%9E%E5%9B%BE%E3%80%82%E4%BE%8B%E5%A6%82%EF%BC%8C%E5%BD%93%E6%88%91%E4%BB%AC%E8%BE%93%E5%85%A5%E8%AE%AD%E7%BB%83%E9%9B%86%E4%B8%AD%E4%B8%8D%E5%AD%98%E5%9C%A8%E7%9A%84%E8%BD%AE%E5%BB%93%E5%9B%BE%E6%97%B6%EF%BC%8C%E9%87%8D%E5%BB%BA%E6%95%88%E6%9E%9C%E4%B8%8Dok%0A-%20generator%3A%20Unet%0A-%20discriminitor%3A%20proposed%20**patchGAN**%2C%20%E6%8A%8A%E5%9B%BE%E5%83%8F**%E7%AD%89%E5%88%86%E6%88%90patch**%EF%BC%8C%E5%88%86%E5%88%AB%E5%88%A4%E6%96%AD%E6%AF%8F%E4%B8%AAPatch%E7%9A%84%E7%9C%9F%E5%81%87%EF%BC%8C%E6%9C%80%E5%90%8E%E5%86%8D**%E5%8F%96%E5%B9%B3%E5%9D%87%EF%BC%81**%E4%BD%9C%E8%80%85%E6%9C%80%E5%90%8E%E8%AF%B4%EF%BC%8C%E6%96%87%E7%AB%A0%E6%8F%90%E5%87%BA%E7%9A%84%E8%BF%99%E4%B8%AAPatchGAN%E5%8F%AF%E4%BB%A5%E7%9C%8B%E6%88%90%E6%89%80%E4%BB%A5**%E5%8F%A6%E4%B8%80%E7%A7%8D%E5%BD%A2%E5%BC%8F%E7%9A%84%E7%BA%B9%E7%90%86%E6%8D%9F%E5%A4%B1%E6%88%96%E6%A0%B7%E5%BC%8F%E6%8D%9F%E5%A4%B1**%E3%80%82%E5%9C%A8%E5%85%B7%E4%BD%93%E5%AE%9E%E9%AA%8C%E6%97%B6%EF%BC%8C%E4%B8%8D%E5%90%8C%E5%B0%BA%E5%AF%B8%E7%9A%84patch%EF%BC%8C%E6%9C%80%E5%90%8E%E5%8F%91%E7%8E%B0**70x70%E7%9A%84%E5%B0%BA%E5%AF%B8%E6%AF%94%E8%BE%83%E5%90%88%E9%80%82**%E3%80%82%0A-%20%E5%AF%B9%E6%AF%94%E6%9C%89%E6%97%A0L1%20loss%E6%97%B6%E7%94%9F%E6%88%90%E5%9B%BE%E5%83%8F%E7%9A%84%E5%8C%BA%E5%88%AB%0A%0A%23%23%23%23%20CycleGAN%EF%BC%8C2017%0ACycleGAN%E6%9C%89%E4%B8%80%E4%BA%9B%E9%9D%9E%E5%B8%B8%E6%9C%89%E8%B6%A3%E7%9A%84%E7%94%A8%E4%BE%8B%EF%BC%8C%E4%BE%8B%E5%A6%82**%E5%B0%86%E7%85%A7%E7%89%87%E8%BD%AC%E6%8D%A2%E4%B8%BA%E7%BB%98%E7%94%BB**%EF%BC%8C%E5%B0%86%E5%A4%8F%E5%AD%A3%E6%8B%8D%E6%91%84%E7%9A%84%E7%85%A7%E7%89%87%E8%BD%AC%E6%8D%A2%E4%B8%BA%E5%86%AC%E5%AD%A3%E6%8B%8D%E6%91%84%E7%9A%84%E7%85%A7%E7%89%87%EF%BC%8C%E6%88%96%E5%B0%86%E9%A9%AC%E7%9A%84%E7%85%A7%E7%89%87%E8%BD%AC%E6%8D%A2%E4%B8%BA%E6%96%91%E9%A9%AC%E7%85%A7%E7%89%87%EF%BC%8C%E6%88%96%E8%80%85%E7%9B%B8%E5%8F%8D%E3%80%82CycleGAN**%E7%94%A8%E4%BA%8E%E4%B8%8D%E5%90%8C%E7%9A%84%E5%9B%BE%E5%83%8F%E5%88%B0%E5%9B%BE%E5%83%8F%E7%BF%BB%E8%AF%91**%E3%80%82%0A%3Epix2pix%E6%98%AFpaired%EF%BC%8Ccyclegan%E6%98%AFunpaired%0A%3Einput%20is%20not%20random%20latent%20vector%0A%3Einstance%20normalization%20%26%20patchGAN%0A%3E%E5%9B%9B%E4%B8%AAloss%20function%0A%0A!%5B29c2819a6fad5056513f9aca0ea75890.png%5D(en-resource%3A%2F%2Fdatabase%2F1033%3A1)%0A!%5Bfc05540bf56c586dc89ddd6718162ac5.png%5D(en-resource%3A%2F%2Fdatabase%2F1035%3A1)%0A%0A%0A%23%23%23%23%20cGAN%0A%E7%B4%A7%E9%9A%8F%E7%9D%80%E5%8E%9F%E7%94%9FGAN%E5%87%BA%E7%8E%B0%E7%9A%84%E3%80%82%E5%9C%A8%E8%BF%99%E7%AF%87%E6%96%87%E7%AB%A0%E4%B8%AD%EF%BC%8C%E4%BD%9C%E8%80%85%E5%9C%A8**%E8%BE%93%E5%85%A5%E7%9A%84%E6%97%B6%E5%80%99%E5%8A%A0%E5%85%A5%E4%BA%86%E6%9D%A1%E4%BB%B6**%EF%BC%88%E7%B1%BB%E5%88%AB%E6%A0%87%E7%AD%BE%E6%88%96%E8%80%85%E5%85%B6%E4%BB%96%E6%A8%A1%E6%80%81%E7%9A%84%E4%BF%A1%E6%81%AF%EF%BC%89%EF%BC%8C%E6%AF%94%E5%A6%82%E5%9C%A8MNIST%E8%AE%AD%E7%BB%83%E5%A5%BD%E7%9A%84%E7%BD%91%E7%BB%9C%EF%BC%8C%E5%8F%AF%E4%BB%A5**%E6%8C%87%E5%AE%9A%E7%94%9F%E6%88%90%E6%9F%90%E4%B8%80%E4%B8%AA%E5%85%B7%E4%BD%93%E6%95%B0%E5%AD%97%E7%9A%84%E5%9B%BE%E5%83%8F%EF%BC%8C%E8%BF%99%E5%B0%B1%E6%88%90%E4%BA%86%E6%9C%89%E7%9B%91%E7%9D%A3%E7%9A%84GAN**%E3%80%82%E5%90%8C%E6%97%B6%EF%BC%8C%E5%9C%A8%E6%96%87%E7%AB%A0%E4%B8%AD%EF%BC%8C%E4%BD%9C%E8%80%85%E8%BF%98%E4%BD%BF%E7%94%A8%E7%BD%91%E7%BB%9C%E8%BF%9B%E8%A1%8C%E4%BA%86**%E5%9B%BE%E5%83%8F%E8%87%AA%E5%8A%A8%E6%A0%87%E6%B3%A8**%E3%80%82%0A%0A%E4%BE%8B%E5%A6%82MNIST%EF%BC%8Cinput%20vector%3D%5Brandom%20latent%20vector%5D%20%2B%20%5Bclass%20label%20vector%5D%0A%E5%9C%A8%E5%8E%9F%E7%94%9FGAN%E4%B8%AD%EF%BC%8C%E5%88%A4%E5%88%AB%E5%99%A8%E7%9A%84%E8%BE%93%E5%85%A5%E6%98%AF%E8%AE%AD%E7%BB%83%E6%A0%B7%E6%9C%ACx%EF%BC%8C%E7%94%9F%E6%88%90%E5%99%A8%E7%9A%84%E7%9A%84%E8%BE%93%E5%85%A5%E6%98%AF%E5%99%AA%E5%A3%B0z%EF%BC%8C%E5%9C%A8conditional%20GAN%E4%B8%AD%EF%BC%8C%E7%94%9F%E6%88%90%E5%99%A8%E5%92%8C%E5%88%A4%E5%88%AB%E5%99%A8%E7%9A%84%E8%BE%93%E5%85%A5%E9%83%BD%E5%A4%9A%E4%BA%86%E4%B8%80%E4%B8%AAy%EF%BC%8C%E8%BF%99%E4%B8%AAy%E5%B0%B1%E6%98%AF%E9%82%A3%E4%B8%AA%E6%9D%A1%E4%BB%B6%E3%80%82%E4%BB%A5%E6%89%8B%E5%86%99%E5%AD%97%E7%AC%A6%E6%95%B0%E6%8D%AE%E9%9B%86MNIST%E4%B8%BA%E4%BE%8B%EF%BC%8C%E8%BF%99%E6%97%B6%E5%80%99x%E4%BB%A3%E8%A1%A8%E5%9B%BE%E7%89%87%E5%90%91%E9%87%8F%EF%BC%8Cy%E4%BB%A3%E8%A1%A8%E5%9B%BE%E7%89%87%E7%B1%BB%E5%88%AB%E5%AF%B9%E5%BA%94%E7%9A%84label(one-hot%E8%A1%A8%E7%A4%BA%E7%9A%840~9)%E3%80%82%0A!%5B7c947df38ef9234d9dde60e9e034670b.png%5D(en-resource%3A%2F%2Fdatabase%2F1019%3A1)%0A%0A%0A%23%23%23%23%20Age-cGAN%0A%E9%9D%A2%E9%83%A8%E8%80%81%E5%8C%96%E6%9C%89%E8%AE%B8%E5%A4%9A%E8%A1%8C%E4%B8%9A%E7%94%A8%E4%BE%8B%EF%BC%8C%E5%8C%85%E6%8B%AC%E8%B7%A8%E5%B9%B4%E9%BE%84%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%EF%BC%8C%E5%AF%BB%E6%89%BE%E5%A4%B1%E8%B8%AA%E5%84%BF%E7%AB%A5%EF%BC%8C%E6%88%96%E8%80%85%E7%94%A8%E4%BA%8E%E5%A8%B1%E4%B9%90%E3%80%82%E8%AE%BA%E6%96%87%E4%B8%AD%E6%8F%90%E5%87%BA%E4%BA%86**%E7%94%A8%E6%9D%A1%E4%BB%B6GAN%E8%BF%9B%E8%A1%8C%E9%9D%A2%E9%83%A8%E8%80%81%E5%8C%96**%E3%80%82%0A%0A%E5%9F%BA%E4%BA%8Elatent%20vector%26generator%26discriminator%E7%9A%84%E5%8F%98%E5%BD%A2%EF%BC%8C%E5%A4%9A%E6%AC%A1%E7%9A%84%E5%AE%9E%E9%AA%8C%E5%B0%9D%E8%AF%95%EF%BC%8C%E6%89%BE%E5%88%B0%E6%9C%80%E4%BD%B3%E7%BB%93%E6%9E%84%EF%BC%8C%E7%BB%99%E4%BA%88%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E7%9A%84%E4%B8%BA%E4%BD%95%E8%83%BD%E5%AE%9E%E7%8E%B0%E6%8C%87%E5%AE%9A%E5%BA%94%E7%94%A8%E7%9A%84%E5%90%88%E7%90%86%E8%A7%A3%E9%87%8A%0A%0A-%20condition%20is%20a%20vector%20represent%20age%0A-%20modify%20to%20generator%0A%0A!%5B163449e16e772fb4f3d31b2da1a77dca.png%5D(en-resource%3A%2F%2Fdatabase%2F1021%3A1)%0A%0A</center></span>
</div></body></html> 